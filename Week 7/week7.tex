%! TEX root = /Users/manunavjeevan/Documents/UCLA/Third Year/Reading Group/wcep.tex

\subsection{Glivenko-Cantelli}%

This subsection follows Section 2.4 in Van DerVaart and Wellner. Goal is to establish conditions for a uniform law of large numbers using bracketing and covering numbers.

\begin{theorem}[Theorem 2.4.1 VdV\&W]
	\label{thm:vdv2.4.1}
	Let \(\calF\) be a class of measurable functions such that  \[\calN_{[\hspace{0.1em}]}\left(\eps,\calF,L_1(P)\right)<\infty\] for every \(\eps>0\). Then  \(\calF\) is Glivenko-Cantelli. 
\end{theorem}
\begin{proof}
	Fix \(\eps > 0\). Choose finitely many \(\eps\)-brackets \([l_i,u_i]\) whose union contains \(\calF\) and such that \(P(u_i-l_i)<\eps\) for every \(i\). Then, for every \(f\in\calF\) there is a bracket, \(l_i \leq f \leq u_i \), such that:
	\begin{align*}
		\left(\P_n-P\right)f \leq \P_n u_i - Pf\leq \left(\P_n-P\right)u_i + P\left(u_i - f\right) \leq \left(\P_n-P\right)u_i + \eps \\ 
		\left(\P_n-P\right)f \geq \P_n l_i - Pf\geq \left(\P_n-P\right)l_i + P\left(l_i - f\right) \geq \left(\P_n-P\right)l_i - \eps 
	\end{align*}
	Consequently,
	\begin{align*}
			\sup_{f\in\calF}\left(\P_n-P\right)f &\leq \max_i \left(\P_n-P\right)u_i + \eps \\
			\inf_{f\in\calF}\left(\P_n-P\right)f &\geq \min_i \left(\P_n-P\right)l_i -\eps
	\end{align*}
	By the strong law of large numbers, both the maximums and the minimums on the right hand side of the inequalities above converge almost surely to 0. Combination these yields that \(\lim\sup\left\|\P_n-P\right\|^\star_\calF \leq \eps\) almost surely for every \(\eps > 0\). Take  \(\eps\downarrow 0\) to see that the \(\limsup\) must be  \(0\) almost surely. 
\end{proof}
\begin{remark*}
	Some comments on Theorem~\ref{thm:vdv2.4.1}:
	\begin{enumerate}
		\item Proof is really quite straightforward. Bracketing gives pointwise control so just use the upper and lower bounds.
		\item No measurability condition is needed and no requirements on the rate of growth of \(\calN_{[\hspace{0.1em}]}\left(\eps,\cdot,\cdot\right)\) as \(\eps\downarrow 0\).
	\end{enumerate}
\end{remark*}
\begin{example*}[Empirical CDF is Glivenko-Cantelli]
	Let \(X\) be a scalar random variable.\footnote{This generalizes easily for a vector valued random variable}. We want to show that 
	\[
		\sup_{t\in\SR}\bigg| \frac{1}{n}\sum_{i=1}^n \mathds{1}\{X_i\leq t\}-P(X_i\leq t)\bigg|=o_p(1)
	.\]
	Let \(\calF = \left\{f(x)=\mathds{1}\{X_i\leq t\}: t \in \SR\right\}\). Partition \(\SR\) into grids  \(-\infty = t_0 < t_1 < \dots< t_m = \infty\) such that \(\P\left(t_i \leq X \leq t_{i+1}\right)<\eps\) for each \(i\). Then the finitely many brackets \(\left[\mathds{1}\{X_i \leq t_i\},\mathds{1}\{X_i \leq t_{i+1}\}\right]\) cover \(\calF\) and are ``size'' \(\eps\) under \(P\). So, \(\calN_{[\hspace{0.1em}]}\left(\eps,\calF,L_1(P)\right)<\infty\) for every \(\eps>0\). So \(\calF\) is Glivenko-Cantelli (i.e, we have a uniform law of large numbers).
\end{example*}
The requirement on the bracketing numbers can in general be hard. Would like a result for the covering numbers as well. This will make showing that some classes are Glivenko-Cantelli easier later on. Before doing so, we need to make a couple definitions:
\begin{definition}[Envelope]
	\label{def:envelope}
	A class \(\calF\) has envelope \(F\) if \(|f(x)|\leq F(x)\) for all \(x\) and all \(f\in\calF\).
\end{definition}
\begin{definition}[Truncated Class]
	\label{def:truncated-class}
	Let \(\calF\) be a class of functions. Then the truncated class \(\calF_M\) is given 
	\[
		\calF_M = \left\{f(x)\mathds{1}\{f \leq M\}: f\in\calF\right\}	
	.\]
\end{definition}
\begin{definition}[P-Measurable Class]
	\label{def:p-measurable-class}
	A class \(\calF\) is \(P\)-measurable if \(\sup_{f\in\calF}\left|\frac{1}{n}\sum_{i=1}^n f(x_i)\eps_i\right|\) is measurable with respect to the product measure on \((\calX^m,\calA^m,P^n)\times(\calZ,\calC,Q)\), where \((\calZ,\calC,Q)\) denotes the probability space that the Rademachar random variables are defined on.
\end{definition}
\begin{definition}[\(L_p(\P_n)\)-norm]
	\label{def:lpn-norm}
	We have that \(\left\|f-g\right\|_{L_1(P)} = \E_P\left[\left|f(x)-g(x)\right|^p\right]^{1/p}\), similarly we can define 
	\[
		\left\|f-g\right\|_{L_p(\P_n)} = \E_{\P_n}\left[\left|f(x)-g(x)\right|^p\right]^{1/p}
	.\]
	and through this define \(\calN_{[\hspace{0.1em}]}\left(\eps,\calF,L_p(\P_n)\right)\). 
\end{definition}

\begin{theorem}[Theorem 2.4.3, VdV\&W]
	\label{thm:vdv2.4.3}
	Let \(\calF\) be a  \(P\)-measurable class of measurable functions with envelope \(F\) such that \(\P^\star F<\infty\). If  \(\log\calN\left(\eps,\calF_M,L_1\left(\P_n\right)\right)= o_{P^\star}(n)\) for every \(\eps\) and  \(M > 0\), then  \(\left\|\P_n-P\right\|^\star_\calF\to 0\) almost surely and in mean. 		
\end{theorem}

\begin{proof}
	Idea will be to apply the maximal inequality in Theorem~\ref{thm:vdv2.2.4}.

	\textit{Step 1: Symmterization.} First, we will apply symmetrization (Corollary~\ref{corr:symmetrization})
	\begin{align*}
		\E^\star\left[\sup_{f\in\calF}\bigg|\frac{1}{n}\sum_{i=1}^n f(X_i)-Pf(X_i)\bigg|\right] 
		&\leq 2\cdot\E^\star\left[\sup_{f\in\calF}\bigg|\frac{1}{n}\sum_{i=1}^n \eps_if(X_i)\bigg|\right] \\
		\intertext{And then truncate the functions, \(f = f\mathds{1}\{f\leq M\} + f\mathds{1}\{f > M\} \), apply triangle inequality, and bound the functions not in \(\calF_M\) with the envelope \(F\).}
		&\leq  2\E_X\E_\eps \left[\sup_{f\in\calF_M}\bigg|\frac{1}{n}\sum_{i=1}^n
		\eps_if(X_i) \bigg|\right] + 2\E^\star\left[\eps_iF(X_i)\mathds{1}\{F \geq M\}\right]	\\
		&=  2\E_X\E_\eps \left[\sup_{f\in\calF_M}\bigg|\frac{1}{n}\sum_{i=1}^n
		\eps_if(X_i) \bigg|\right] + 2P^\star F(X_i)\mathds{1}\{F \geq M\}	
	\end{align*}
	Note the argument that allows us to replace the first outer expectation with iterated expectations over \(X\) and  \(\eps\): each of the functions in \(\calF\) are measurable and  \(\calF_M\) is uniformly bounded, which means that the supremum will be measurable and bounded with probability 1 in any finite sample (with respect to the empirical measure/conditional on the \(X\) data).

	Since \(P^\star F <\infty\) we can choose \(M\) so that the term on the right is arbitrarily small. \footnote{ I am sort of using \(P^\star\) and  \(\E^\star_X\) interchangeably here, which I apologize for } That is, for any \(\delta > 0\) we can pick  \(M\) such that
	\[
		\E^\star\left[\sup_{f\in\calF}\bigg|\frac{1}{n}\sum_{i=1}^n f(X_i)-Pf(X_i)\bigg|\right] \leq \E_X\E_\eps\left[\sup_{f\in\calF_M}\bigg|\frac{1}{n}\sum_{i=1}^n \eps_if(X_i)\bigg|\right] + \delta
	.\]

	\textit{Step 2: Deal with the term that is conditional on \(\{X_i\}\).} Let \(\calG_\delta = \{g_1,\dots,g_{K(\delta)}\}\) be such that for every \(f\in\calF_M\) there is a  \(g \in \calG_\delta\) such that \(\left\|f-g\right\|_{L_1(\P_n)}<\delta\). Since \(\log\calN\left(\delta,\calF_m,L_1(\P_n)\right)=o_p(n)\), we know that it is possible to pick a \(\calG_\delta\) in this fashion with probability approaching 1. Note that:
	\begin{itemize}
		\item Cardinality of \(\calG_\delta\): \(\left|\calG_\delta\right| = \calN\left(\delta,\calF_M,\|\cdot\|_{L_1(\P_n)}\right).\)
		\item Envelope of \(\calG_\delta\): by construction \(\calF_M \leq M\) so we can assume that \(\calG_\delta\leq M\).
	\end{itemize}
	Then, for all \(f\in\calF_M\) we have that, for some  \(g\in\calG_\delta\):
	\begin{align*}
		\bigg|\frac{1}{n}\sum_{i=1}^n \eps_if(X_i)\bigg|
		&\leq \bigg|\frac{1}{n}\sum_{i=1}^n \eps_ig(X_i)\bigg| + \bigg|\frac{1}{n}\sum_{i=1}^n \eps_i\left(f(X_i) - g(X_i)\right)\bigg| \\
		&\leq \bigg|\frac{1}{n}\sum_{i=1}^n \eps_ig(X_i)\bigg|+\delta 
	\end{align*}
	This gives us that
	\[
		\E_\eps\left[\sup_{f\in\calF_M}\bigg|\frac{1}{n}\sum_{i=1}^n \eps_if(X_i)\bigg|\right] \leq \E_\eps\left[\sup_{g\in\calG_\delta}\bigg|\frac{1}{n}\sum_{i=1}^n \eps_ig(X_i)\bigg|\right]+\delta
	.\]
	\textit{Step 3: Apply the Maximal Inequality}. We bound the first term in the last display using the maximal inequality in Lemma~\ref{lemma:vdv2.2.2}, for the particular case of the \(\psi_2\) Orlicz norm \footnote{We know that, in any finite sample, this Orlicz norm exists because our empirical expectation is bounded.}: if \(D = \{f_1,\dots,f_M\}\) then 
	\[\E\left[\sup_{f\in D}\bigg|f(X_i)\bigg|\right] \leq C\sqrt{1+\log m}\]
	for any \(C\) with  \(\E\left[\exp\left(\frac{f(X_i)}{C^2}\right)-1\right]\leq 1\) for all \(f\in D\). In our setting we will apply this to the functions \(\frac{1}{n}\sum_{i=1}^n \eps_i g(X_i)\) for \(g\in\calG_\delta\), with the \(g(X_i)\) treated as fixed so that  these are considered random variables in \(\eps_i\). In our setting we can bound:
	\[
		\E_\eps\left[\sup_{g\in\calG_\delta}\bigg|\frac{1}{n}\sum_{i=1}^n \eps_ig(X_i)\bigg|\right] \leq C_\delta \sqrt{1 + \log\calN\left(\delta,\calF_M,\|\cdot\|_{L_1(\P_n)}\right)}
	.\] 
	for such a \(C_\delta\) such that, for all \(g \in \calG_\delta\)
	 \[
		 \E_\eps\left[\exp\left(\bigg(\frac{1}{n}\sum_{i=1}^n \eps_ig(X_i)\bigg)^2/C_\delta^2\right)-1\right]\leq 1
	.\] 
	Hoeffding's inequality (Lemma~\ref{lemma:hoeffding}) gives us that, for a general Rademachar process:
	\begin{align*}
		\P_\eps\left(\bigg|\sum_{i=1}^n \eps_ia_i\bigg|>x\right) &\leq 2\exp\left(-\frac{1}{2}\frac{x^2}{\|a\|^2}\right)\\
		\intertext{Where the norm above is the standard Euclidean norm. In our setting:}
		\P_\eps\left(\bigg|\sum_{i=1}^n \eps_i\frac{g(X_i)}{n} \bigg| > x\right) &\leq 2\exp\left(-\frac{1}{2}\frac{x^2}{n^{-2}\sum g(X_i)^2} \right)\\
		\intertext{As discussed above, we can uniformly bound  \(\calG_\delta\) by  \(M\). The exponential is negative so it is decreasing in the numerator and increasing in the denominator. This allows us to get:} 
		\P_\eps\left(\bigg|\sum_{i=1}^n \eps_i\frac{g(X_i)}{n} \bigg| > x\right) &\leq 2\exp\left(-\frac{n x^2}{2M^2} \right)
	\end{align*}
	Now apply Lemma~\ref{lemma:vdv2.2.1}. If \(\P\left(|X|>x\right)\leq Ke^{-Dx^2}\) then the \(\psi_2\)-Orlicz norm of \(X\) is less than  \(\sqrt{(1+K)/D}\). Using the above take \(K=2\) and \(D = n/(2M^2)\) to get \(C_\delta = \sqrt{6M^2/n}\). Putting this together with the original application of the maximal inequality towards the top of Step 3, we get:
	\begin{align*}
		\E_\eps\left[\sup_{g\in\calG_\delta}\bigg|\frac{1}{n}\sum_{i=1}^n \eps_ig(X_i)\bigg|\right]&\leq \sqrt{6M^2/n + \log\calN\left(\delta,\calF_M,\|\cdot\|_{L_1(\P_n)}\right)/n}
	\end{align*}
	By assumption \(\log\calN(\cdot)/n = o_P(1)\) and the first term under the square root is \(o(1)\) so that this whole thing is \(o_p(1)\). All together, combining this with the result of Step 2, we have shown that, for any \(\delta > 0\):
	\[
		\E_\eps\left[\sup_{f\in\calF_M}\bigg|\frac{1}{n}\sum_{i=1}^n \eps_if(X_i)\bigg|\right] \leq \delta + o_p(1) 
	.\] 
	So that the whole thing (\(\E_\eps[\sup_{f\in\calF_M}\dots]\)) is \(o_p(1)\). 

	\textit{Step 4: Put all the parts together.}
	
	By the symmetrization at the top of step 1, we have that, for every \(\delta > 0\)
	 \[
		 \E\left[\sup_{\calF}\bigg|\frac{1}{n}\sum_{i=1}^n \eps_if(X_i) \bigg|\right] \leq  2\E_X\E_\eps\left[\sup_{f\in\calF_M}\bigg|\frac{1}{n}\sum_{i=1}^n \eps_if(X_i)\bigg|\right] + \delta 
	.\] 
	In Step 3, we showed that the inner expectation is \(o_p(1)\). Combining this with the fact that \(\calF_M \leq M\) gives us that\footnote{Convergence in probability to 0 implies convergence in distribution to 0 implies convergence in bounded moments}:
	\[
		\E_X\E_\eps\left[\sup_{f\in\calF_M}\bigg|\frac{1}{n}\sum_{i=1}^n \eps_if(X_i)\bigg|\right] = o(1)
	.\] 
	which we can combine with Markov's  inequality to get that
	\[
		\sup_{f\in\calF}\bigg|\frac{1}{n}\sum_{i=1}^n \left\{f(X_i) - Pf(X_i)\right\}\bigg| = o_p(1)
	.\]
	This shows that \(\|\P_n-P\|_\calF^\star \to 0\) in mean. From VdV\&W: That it also converges almost surely follows from the fact that the sequence \(\|\P_n-P\|_\calF^\star\) is a reverse martingale with respect to a suitable filtration.\footnote{This part may depend on i.i.d. I am not familiar with the martingale convergence theorems.}  
\end{proof}

\begin{remark*}
	A couple comments on Theorem~\ref{thm:vdv2.4.3}:
	\begin{itemize}
		\item Proof is harder than that using bracketing numbers (Theorem~\ref{thm:vdv2.4.1}). However, the technique is much closer to what will be used for the Donsker Theorems. 
		\item Note how the measurability is obtained using the \(\eps_i\) and conditioning on  \(\{X_i\}\).
		\item The conditions look cryptic, but we will find ways of verifying them.
	\end{itemize}
\end{remark*}

\subsection{Donsker Theorems}%
\label{subsec:Donsker}

This subsection follows section 2.5 in Van DerVaart and Wellner. In this subsection we will establish conditions for \(\calF\) to be Donsker (Definition~\ref{def:donsker-class}). We will present two main results, one that relies on the covering numbers (through a Uniform Entropy condition) and another using the bracketing numbers. 

\begin{definition}[Uniform Entropy Condition]
	\label{def:uniform-entropy}
	Let \(\calF\) be a class of functions with envelope \(F\) and let \(\calQ\) be the set of all finitely discrete probability measures on  \((\calX,\calA)\). We say that \(\calF\) satisfies a uniform entropy bound if:
	\begin{equation}
		\label{eq:uniform-entropy-bound}
		\int_{0}^{\infty} \sup_{Q\in\calQ} \sqrt{\log\left(\eps\|F\|_{Q,2},\calF, L_2(Q)\right)}\,d\eps<\infty\tag{UEB}
	\end{equation}
\end{definition}
Similarly to before define
\begin{align}
	\calF_\delta &= \left\{f-g:f,g\in\calF\text{ and }\|f-g\|_{P,2}<\delta\right\} \\
	\calF_\infty^2 &= \left\{(f-g)^2: f,g\in\calF\right\}
\end{align}
\begin{theorem}[Theorem 2.5.2 VdV\&W]
	\label{thm:vdv2.5.2}
	Let \(\calF\) be a class of measurable functions with envelope \(F\) that satisfies the uniform entropy bound, \eqref{eq:uniform-entropy-bound}. Let the classes \(\calF_\delta\) and  \(\calF_\infty^2\) also be  \(P\)-measurable for every  \(\delta >0\). If  \(P^\star F^2 <\infty\) then \(\calF\) is  \(P\)-Donsker.
\end{theorem}
\begin{proof}
	Because the envelope \(F\) has a bounded second moment we can apply CLT to get convergence of the marginals for any finite collection \(f_1,\dots,f_k\). That is
	\[
		\left(\mathbb{G}_n f_1,\dots,\mathbb{G}_n f_n\right) \overset{L}{\to}\left(\mathbb{G}f_1,\dots,\mathbb{G}f_k\right)
	.\] 
	for some tight limit \(\mathbb{G}\). By Theorem~\ref{thm:vdv1.5.4} it is now sufficient (and necessary) to show that \(\mathbb{G}_n(\calF)\) is asymptotically tight. We do this by way of Theorem~\ref{thm:vdv1.5.7}, showing that \(\mathbb{G}_n\) is asymptotically \(\rho\)-equicontinuous and \(\calF\) it totally bounded for some \(\rho\).\footnote{We know that the marginals of \(\mathbb{G}_n\) are asymptotically tight because they converge to a tight limit (Lemma~\ref{lemma:vdv1.3.8})} That is, we want to show that there is some \(\rho\) semimetric with 
	 \[
		 \P\left(\sup_{\rho(f,g)<\delta}\big|\mathbb{G}_nf-\mathbb{G}_ng\big|>\eps\right)<\eta
	.\] 
	
	Goal will be to show this for \(\rho(f,g) = \left(P(f-g)^2\right)^{1/2}\). 

	\textit{Step 1: Use Symmetrization to apply Maximal Inequality.} Apply Markov's inequality and then Lemma~\ref{lemma:vdv2.3.1} (or Corollary~\ref{corr:symmetrization}) to the class \(\sqrt{n}\calF_\delta\) to get:
	\begin{align*}
		\P\left(\sup_{\rho(f,g)<\delta}\left|\mathbb{G}_nf-\mathbb{G}_ng\right|>\eps\right)
		&\leq\frac{1}{\eps}\cdot\E\left[\sup_{\rho(f,g)<\delta}\left|\mathbb{G}_nf-\mathbb{G}_ng\right|\right] \\
		&\leq \frac{2}{\eps}\cdot\E_X\E_\eps\left[\sup_{\rho(f,g)<\delta}\bigg|\frac{1}{\sqrt{n}}\sum_{i=1}^n \eps_i\left(f(x_i)-g(x_i)\right)\bigg|\right]\tag{D-0}\label{eq:donsker0}
	\end{align*}
	Note that the inside is measurable, so we can use iterated expectations. Recall by the maximal inequality (Theorem~\ref{thm:vdv2.2.4}) if \(D\) is a set equipped with metric  \(d\) and  \(\mathbb{G}\) is a subgaussian process, then
	\[
		\E\left[\sup_{f,g\in D}\left|\mathbb{G}f-\mathbb{G}g\right|\right] \leq C \int_{0}^{\text{diam}(D)}\sqrt{\log\calN(\eps,D,d)}\;d\eps  
	.\]
	where subgaussian is defined as \(\P\left(\left|\mathbb{G}f-\mathbb{G}f\right|>x\right) \leq  2\exp\left(-\frac{1}{2}x^2/d^2(f,g)\right)\). Lemma~\ref{lemma:hoeffding} (Hoeffding's) gives us that the Rademachar process is subgaussian conditional on \(\{X_i\}\) for any class of functions \(\calF\) equipped with the  \(L_2(\P_n)\) norm.\footnote{Note that \[
			\left\|\left[\frac{1}{\sqrt n}f(x_i)-\frac{1}{\sqrt n}g(x_i)\right]_{i=1}^n\right\|^2 = \frac{1}{n}\sum_{i=1}^n \left(f(x_i)-g(x_i)\right)^2 = \left\|f-g\right\|_ {L_2(\P_n)}
	.\] } This gives us that
	\begin{equation}
		\label{eq:donsker1}
		\E_\eps\left[\sup_{\rho(f,g)<\delta}\bigg|\frac{1}{\sqrt n}\sum_{i=1}^n \eps_i\left(f(x_i)-g(x_i)\right)\bigg|\right]\lesssim \int_{0}^{\text{diam}(\calF_\delta)} \sqrt{\log\calN\left(\eps,\calF_\delta,\|\cdot\|_{L_2(\P_n)}\right)}\;d\eps\tag{D-1} 
	\end{equation}
	where note that the diameter on the RHS is calculated with respect to \(L_2(P_n)\) not the \(\rho(f,g) = \left|f-g\right|_{L_2(P)}\) on the right hand side.\footnote{It also may be helpful to recall that \(\calF_\delta =\left\{f-g:\rho(,g)<\delta\right\}\)} 

	\textit{Step 2: Make Sense of the Upper Bound.} Let \(\theta_n^2 := \text{diam}^2(\calF_\delta) = \sup_{\rho(f,g)<\delta}\frac{1}{n}\sum_{i=1}^n \left(f(x_i)-g(x_i)\right)^2\). Let \(u := \eps/\left\|F\right\|_{L^2(\P_n)}\) and rewrite the above\footnote{Going to use \(L_2(\P_n)\) or just \(\|\cdot\|_{\P_n,2}\) instead of  \(\|\cdot\|_{L_2(\P_n)}\) to save some space. Also the empirical measure is bounded and \(F\) has bounded second outer moment so we know that  \(\|F\|_{L_2(\P_2)}<\infty\) almost surely.}
	\begin{align*}
		\int_{0}^{\text{diam}(\calF_\delta)}\sqrt{\log\calN(\eps,\calF_\delta,L_2(\P_n))}\;d\eps 
		&= \int_{0}^{\theta_n} \sqrt{\log\calN(\eps,\calF_\delta,L_2(\P_n))}\;d\eps\\
		&= \|F\|_{\P_n,2} \int_{0}^{\theta_n/\left\|F\right\|_{\P_n,2}} \sqrt{\log\calN\big(u\left\|F\right\|_{\P_n},\calF_\delta,L_2(\P_n)\big)}\;du
		\intertext{Since \(\P_n\) is a discrete probability measure}
		&\leq \|F\|_{\P_n,2}\int_0^{\theta/\left\|F\right\|_{\P_n,2}}\sup_{\calQ}\sqrt{\log\calN\big(u\|F\|_{Q,2},\calF,\|\cdot\|_{L^2(Q)}\big)}\;du
		\intertext{But, since \(\calF_\delta\subseteq\calF_\infty\) we get that \(\calN(\eps,\calF_\delta,L_2(Q))\leq \calN(\eps,\calF_\infty,L_2(Q))\). Also, see that \(\calN(\eps,\calF_\infty,L_2(Q))\leq \calN^2(\eps/2,\calF,L_2(Q))\). ({Why?}) In total:} 
		\int_{0}^{\text{diam}(\calF_\delta)}\sqrt{\log\calN(\eps,\calF_\delta,L_2(\P_n))}\;d\eps &\leq \|F\|_{\P_n,2}\int_{0}^{\theta_n/\|F\|_{\cdot}}\sup_{\calQ}\sqrt{2\log\calN\big(u\|F\|_{Q,2}/2,\calF,L_2(Q)\big)}\;du \tag{D-2}\label{eq:donsker2}
	\end{align*}

	\textit{Step 3: Go Back to the Full Expectation.} Combining the inequality directly above and~\eqref{eq:donsker1} and applying to the symmetrization inequality in \eqref{eq:donsker0}, we get that 
	\begin{align*}
		\E\left[\sup_{\rho(f,g)<\delta}\bigg|\frac{1}{\sqrt n}\sum_{i=1}^n \eps_i\left(f(x_i)-g(x_i)\right)\bigg|\right] &\leq \E\left[ \|F\|_{\P_n,2}\int_{0}^{\theta_n/\|F\|_{\cdot}}\sup_{\calQ}\sqrt{2\log\calN\big(u\|F\|_{Q,2}/2,\calF,L_2(Q)\big)}\;du \right]
	\end{align*}
	Where the expectations above are with respect to \(X\). Apply Cauchy-Schwarz to upper bound the above by 
	\begin{equation}
		\label{eq:donsker3}
		\tag{D-3}
		\E_X\left[\left(\int_{0}^{\theta_n/\|F\|_{\cdot}}\sup_{\calQ}\sqrt{2\log\calN\big(u\|F\|_{Q,2}/2,\calF,L_2(Q)\big)}\;du\right)^2\right]^{1/2}\E_X\left[\|F\|_{\P_n,2}^2\right]^{1/2}
	\end{equation}
	Note that \(\E_X[\|F\|^2_{\P_n,2}] = \E_X[n^{-1}\sum F^2(x_i)] \leq P^\star F^2 <\infty\). What is left is to show that the expectation of the integral in ~\eqref{eq:donsker3} converges to zero provided that \(\theta_n/\|F\|_{\P_n,2}\to_{P^\star}0\).

	\textit{Step 4: Figure out what is happening with \(\theta_n/\|F\|_{\P_n,2}\).} Note that \(\|F\|_{\P_n,2}\) is bounded below by \(\|F_\star\|_{\P_n,2}\) which converges almost surely to its expectation. Recall that 
	\[\theta_n^2 = \sup_{\rho(f,g)<\delta} \frac{1}{n}\sum_{i=1}^n \left(f(x_i)-g(x_i)\right)^2 \leq \underbrace{\sup_{\rho(f,g)<\delta} \|(\P_n-P)(f-g)^2\|}_{=\|\P_n-P\|_{\calF^2_\delta}} + \underbrace{\sup_{\rho(f,g)<\delta}P(f-g)^2}_{<\delta^2}. \] 
	Since \(\calF^2_{\delta}\subseteq\calF^2_\infty\) we want to show to show that \(\calF_\infty^2\) is Glivenko-Cantelli. Theorem~\ref{thm:vdv2.4.3} gives that is enough to show that \(\log\calN(\eps,\calF_\infty^2,L_1(\P_n)) = o_{P^\star}(n)\).\footnote{Inherits bounded envelope by triangle inequality. If \(F\) bounds  \(\calF\), then  \(4F^2\) bounds  \(\calF^2_\infty\).} For any pair of functions \(f,g\in\calF_\infty\) 
	\[
		\P_n|f^2-g^2| = \P_n|(f-g)(f+g)| \leq \P_n|f-g|4F \leq2\|F\|_{\P_n,2}
	\|f-g\|_{\P_n,2}.\footnote{By Cauchy-Schwarz: \(\frac{1}{n}\sum_{i=1}^n f(x_i)g(x_i)\leq \left[\frac{1}{n}\sum_{i=1}^n f^2(x_i) \right]^{1/2}\left[\frac{1}{n}\sum_{i=1}^n g^2(x_i)\right]^{1/2}\) }\] 
	where in the above we use that \(2F\) is an envelope for \(\calF_\infty\). This gives us that \(\calN(\eps,\calF_\infty^2,L_1(\P_n)) \leq \calN(\eps/\|2F\|_{\P_n,2}),\calF_\infty,L_2(\P_n))\). Why? Suppose we cover \(\calF^2_\infty\) with N balls of size  \(\eps\). By the inequalities above,  we can find equivalent (centered at say, \((f-g)/2\) instead of  \((f^2-g^2)/2\)) balls that will cover \(\calF_\infty^2\) with radius \(\eps/\|F\|_{\P_n,2}\) in \(L_2(\P_n)\). As argued above, we know that this is less than  \(\calN^2(\eps\big/4\|F\|_{L_2(\P_n)},\calF,L_2(\P_n))\) which we know has to be finite for any \(\eps\) in order for \eqref{eq:uniform-entropy-bound} to be satisfied. Since \(\calN(\eps,\calF_\infty,L_1(\P_n))\) is bounded by a constant, it's logarithm is surely \(o_{p^\star}(n)\). 

	So, combining with the display at the beginning of this set we have that
	\[
		\theta_n^2 \leq \sup_{\rho(f,g)<\delta}\bigg|(\P_n-P)(f-g)^2\bigg| + \delta^2 \to_{a.s}\delta
	.\] 
	Together this gives us that 
	\[
		\frac{\theta_n}{\|F\|_{\P_n,2}} \leq \frac{\theta_n}{\|F_\star\|_{\P_n,2}} \to_{a.s}\frac{\delta}{\|F_\star\|_{P,2}} \tag{D-4}\label{eq:donsker4}
	.\] 

	\textit{Step 5: Put Together to get Asymptotic Equicontinuity.} Take \(C = (P^\star F^2)^{1/2}\) and combine the inequalities in  \eqref{eq:donsker1}, \eqref{eq:donsker2}, \eqref{eq:donsker3} with the convergence result in  \eqref{eq:donsker4}.
	\begin{align*}
		\lim\sup\P&\left(\sup_{\rho(f,g)<\delta}\left|\mathbb{G}_nf-\mathbb{G}_ng\right|>\eps\right) \\
		&\leq \frac{1}{\eps}\lim\sup\E\left[ \|F\|_{\P_n,2}\int_{0}^{\theta_n/\|F\|_{\P_n,2}}\sup_{\calQ}\sqrt{2\log\calN\big(u\|F\|_{Q,2}/2,\calF,L_2(Q)\big)}\;du \right] \\
		&\leq \frac{C}{\eps}\lim\sup\E\left[\left( \int_{0}^{\theta_n/\|F\|_{\P_n,2}} \sup_{\calQ}\sqrt{2\log\calN\big(u\|F\|_{Q,2}/2,\calF,L_2(Q)\big)}\;du\right)^2\right]^{1/2}\\
		&\to_{a.s} \frac{C}{\eps}\int_{0}^{\delta/\|F_\star\|_{P,2}} \sup_{\calQ}\sqrt{2\log\calN\big(u\|F\|_{Q,2}/2,\calF,L_2(Q)\big)}\;du
	\end{align*}
	However, by the uniform entropy bound in \eqref{eq:uniform-entropy-bound} the integral up to infinity in the above display is finite so the display above converges to 0 as \(\delta\downarrow 0\). So we have verified that  \(\mathbb{G}_n\) is asymptotically equicontinuous.

	\textit{Step 6: Verify that \(\calF\) is totally bounded.} Finally what remains is to show that \(\calF\) is totally bounded in  \(L_2(P)\) (Definition~\ref{def:totally-bounded}). Take a sequence of discrete measures \(\P_n\) such that  \(\left\|\P_n-P\right\|_{\calF_\infty}\) converges to 0. Since we know that \(\calF_\infty\) is Glivenko-Cantelli, this is always possible. Pick \(n\) large enough such that  \(\|\P_n-P\|_{\calF_\infty} < \delta^2\). By triangle inequality
	\[
		P(f-g)^2 = \P_n(f-g)^2 - (\P_n-P)(f-g)^2\leq \frac{1}{n}\sum_{i=1}^n \left(f(x_i)-g(x_i)\right)^2 + \|\P_n-P\|_{\calF_\infty} 
	.\]
	This implies that \(\calF\) is totally bounded under  \(L_2(P)\) since we are totally bounded under  \(L_2(\P_n)\) (Finite Envelope + Finite Measure).	
\end{proof}

\begin{example}[Cells in \(\SR\) are Donsker]
	\label{ex:cells-donsker}
	Let \(X\) be a scalar and suppose that we want to show a functional CLT for
	 \[
		 \mathbb{G}_n(t) = \frac{1}{\sqrt n}\sum_{i=1}^n \left\{\mathds{1}[X_i\leq t]-P(X\leq t)\right\} 
	.\]
	Main challenge is showing the uniform entropy bound~\eqref{eq:uniform-entropy-bound}. Recall that for any \(\|\cdot\|\), \(\calN(\eps,\calF,\|\cdot\|)\leq \calN_{[\hspace{0.1em}]}(2\eps,\calF,\|\cdot\|)\). Apply the above to \(\|\cdot\|_{Q,2}\). Partition \(\SR\) into  \(-\infty=t_0<t_1<\dots<t_k= \infty\) such that \(\P(t_i\leq X\leq t_{i+1}) \leq 4\eps^2\). These cover \(\calF= \left\{f(x)=\mathds{1}\{x\leq t\}, t\in\SR \right\}\). Further
	\[
		\E\left[\left(\mathds{1}[X\leq t_i]-\mathds{1}[X\leq t_{i+1}]\right)^2\right]^{1/2} = \P\left(t_i\leq X\leq T_{i+1}\right)^{1/2} = 2\eps
	.\]
	In order to cover \(\calF\) we need  \(\lceil 1/4\eps^2\rceil \leq 2\eps^2\). This process can be done for any probability measure \(Q\) so that if \(\eps < 1\):
	 \[
		 \calN(\eps,\calF,L_2(Q)) \leq  \calN_{[\hspace{0.1em}]}(2\eps,\calF,L_2(Q)) \leq 2/\eps^2  \wedge 1
	.\]
	If \(\eps \geq 1\) only one ball is needed. Then, since \(F(x)=1\) and if  \(\eps>1\) one ball is enough (and \(\log 1 =0\)):
	\begin{align*}
		\int_{0}^{\infty} \sup_{\calQ} \sqrt{\log\calN(\eps\|F\|\|_{Q,2},\calF,L_2(Q))}\;d\eps &= \int_{0}^{1} \sup_{\calQ}\sqrt{\log\calN(\eps,\calF,L_2(Q))}\;d\eps \\
											 &\leq \int_{0}^{1} \sqrt{\log (2/\eps)}\;d\eps 
	\end{align*}
	This is easily Donsker by Theorem~\ref{thm:vdv2.5.2} and in fact the argument hold for cells in \(\SR^K\). However, note that this seems like cheating a bit, since we are using the bracketing numbers to get the covering numbers.
\end{example}

