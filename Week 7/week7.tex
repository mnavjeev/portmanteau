\subsection{Glivenko-Cantelli}%

This subsection follows Section 2.4 in Van DerVaart and Wellner. Goal is to establish conditions for a uniform law of large numbers using bracketing and covering numbers.

\begin{theorem}[Theorem 2.4.1 VdV\&W]
	\label{thm:vdv2.4.1}
	Let \(\calF\) be a class of measurable functions such that  \[\calN_{[\hspace{0.1em}]}\left(\eps,\calF,L_1(P)\right)<\infty\] for every \(\eps>0\). Then  \(\calF\) is Glivenko-Cantelli. 
\end{theorem}
\begin{proof}
	Fix \(\eps > 0\). Choose finitely many \(\eps\)-brackets \([l_i,u_i]\) whose union contains \(\calF\) and such that \(P(u_i-l_i)<\eps\) for every \(i\). Then, for every \(f\in\calF\) there is a bracket, \(l_i \leq f \leq u_i \), such that:
	\begin{align*}
		\left(\P_n-P\right)f \leq \P_n u_i - Pf\leq \left(\P_n-P\right)u_i + P\left(u_i - f\right) \leq \left(\P_n-P\right)u_i + \eps \\ 
		\left(\P_n-P\right)f \geq \P_n l_i - Pf\geq \left(\P_n-P\right)l_i + P\left(l_i - f\right) \geq \left(\P_n-P\right)l_i - \eps 
	\end{align*}
	Consequently,
	\begin{align*}
			\sup_{f\in\calF}\left(\P_n-P\right)f &\leq \max_i \left(\P_n-P\right)u_i + \eps \\
			\inf_{f\in\calF}\left(\P_n-P\right)f &\geq \min_i \left(\P_n-P\right)l_i -\eps
	\end{align*}
	By the strong law of large numbers, both the maximums and the minimums on the right hand side of the inequalities above converge almost surely to 0. Combination these yields that \(\lim\sup\left\|\P_n-P\right\|^\star_\calF \leq \eps\) almost surely for every \(\eps > 0\). Take  \(\eps\downarrow 0\) to see that the \(\limsup\) must be  \(0\) almost surely. 
\end{proof}
\begin{remark*}
	Some comments on Theorem~\ref{thm:vdv2.4.1}:
	\begin{enumerate}
		\item Proof is really quite straightforward. Bracketing gives pointwise control so just use the upper and lower bounds.
		\item No measurability condition is needed and no requirements on the rate of growth of \(\calN_{[\hspace{0.1em}]}\left(\eps,\cdot,\cdot\right)\) as \(\eps\downarrow 0\).
	\end{enumerate}
\end{remark*}
\begin{example*}[Empirical CDF is Glivenko-Cantelli]
	Let \(X\) be a scalar random variable.\footnote{This generalizes easily for a vector valued random variable}. We want to show that 
	\[
		\sup_{t\in\SR}\bigg| \frac{1}{n}\sum_{i=1}^n \mathds{1}\{X_i\leq t\}-P(X_i\leq t)\bigg|=o_p(1)
	.\]
	Let \(\calF = \left\{f(x)=\mathds{1}\{X_i\leq t\}: t \in \SR\right\}\). Partition \(\SR\) into grids  \(-\infty = t_0 < t_1 < \dots< t_m = \infty\) such that \(\P\left(t_i \leq X \leq t_{i+1}\right)<\eps\) for each \(i\). Then the finitely many brackets \(\left[\mathds{1}\{X_i \leq t_i\},\mathds{1}\{X_i \leq t_{i+1}\}\right]\) cover \(\calF\) and are ``size'' \(\eps\) under \(P\). So, \(\calN_{[\hspace{0.1em}]}\left(\eps,\calF,L_1(P)\right)<\infty\) for every \(\eps>0\). So \(\calF\) is Glivenko-Cantelli (i.e, we have a uniform law of large numbers).
\end{example*}
The requirement on the bracketing numbers can in general be hard. Would like a result for the covering numbers as well. This will make showing that some classes are Glivenko-Cantelli easier later on. Before doing so, we need to make a couple definitions:
\begin{definition}[Envelope]
	\label{def:envelope}
	A class \(\calF\) has envelope \(F\) if \(|f(x)|\leq F(x)\) for all \(x\) and all \(f\in\calF\).
\end{definition}
\begin{definition}[Truncated Class]
	\label{def:truncated-class}
	Let \(\calF\) be a class of functions. Then the truncated class \(\calF_M\) is given 
	\[
		\calF_M = \left\{f(x)\mathds{1}\{f \leq M\}: f\in\calF\right\}	
	.\]
\end{definition}
\begin{definition}[P-Measurable Class]
	\label{def:p-measurable-class}
	A class \(\calF\) is \(P\)-measurable if \(\sup_{f\in\calF}\left|\frac{1}{n}\sum_{i=1}^n f(x_i)\eps_i\right|\) is measurable with respect to the product measure on \((\calX^m,\calA^m,P^n)\times(\calZ,\calC,Q)\), where \((\calZ,\calC,Q)\) denotes the probability space that the Rademachar random variables are defined on.
\end{definition}
\begin{definition}[\(L_p(\P_n)\)-norm]
	\label{def:lpn-norm}
	We have that \(\left\|f-g\right\|_{L_1(P)} = \E_P\left[\left|f(x)-g(x)\right|\right]\), similarly we can define 
	\[
			\left\|f-g\right\|_{L_{1(\P_n)}} = \E_{\P_n}\left[\left|f(x)-g(x)\right|\right]
	.\]
	and through this define \(\calN_{[\hspace{0.1em}]}\left(\eps,\calF,L_1(\P_n)\right)\).
\end{definition}

\begin{theorem}[Theorem 2.4.3, VdV\&W]
	\label{thm:vdv2.4.3}
	Let \(\calF\) be a  \(P\)-measurable class of measurable functions with envelope \(F\) such that \(\P^\star F<\infty\). If  \(\log\calN\left(\eps,\calF_M,L_1\left(\P_n\right)\right)= o_{P^\star}(n)\) for every \(\eps\) and  \(M > 0\), then  \(\left\|\P_n-P\right\|^\star_\calF\to 0\) almost surely and in mean. 		
\end{theorem}

\begin{proof}
	Idea will be to apply the maximal inequality in Theorem~\ref{thm:vdv2.2.4}.

	\textit{Step 1: Symmterization.} First, we will apply symmetrization (Corollary~\ref{corr:symmetrization})
	\begin{align*}
		\E^\star\left[\sup_{f\in\calF}\bigg|\frac{1}{n}\sum_{i=1}^n f(X_i)-Pf(X_i)\bigg|\right] 
		&\leq 2\cdot\E^\star\left[\sup_{f\in\calF}\bigg|\frac{1}{n}\sum_{i=1}^n \eps_if(X_i)\bigg|\right] \\
		\intertext{And then truncate the functions, \(f = f\mathds{1}\{f\leq M\} + f\mathds{1}\{f > M\} \), apply triangle inequality, and bound the functions not in \(\calF_M\) with the envelope \(F\).}
		&\leq  2\E_X\E_\eps \left[\sup_{f\in\calF_M}\bigg|\frac{1}{n}\sum_{i=1}^n
		\eps_if(X_i) \bigg|\right] + 2\E^\star\left[\eps_iF(X_i)\mathds{1}\{F \geq M\}\right]	\\
		&=  2\E_X\E_\eps \left[\sup_{f\in\calF_M}\bigg|\frac{1}{n}\sum_{i=1}^n
		\eps_if(X_i) \bigg|\right] + 2P^\star F(X_i)\mathds{1}\{F \geq M\}	
	\end{align*}
	Note the argument that allows us to replace the first outer expectation with iterated expectations over \(X\) and  \(\eps\): each of the functions in \(\calF\) are measurable and  \(\calF_M\) is uniformly bounded, which means that the supremum will be measurable and bounded with probability 1 in any finite sample (with respect to the empirical measure/conditional on the \(X\) data).

	Since \(P^\star F <\infty\) we can choose \(M\) so that the term on the right is arbitrarily small. \footnote{ I am sort of using \(P^\star\) and  \(\E^\star_X\) interchangeably here, which I apologize for } That is, for any \(\delta > 0\) we can pick  \(M\) such that
	\[
		\E^\star\left[\sup_{f\in\calF}\bigg|\frac{1}{n}\sum_{i=1}^n f(X_i)-Pf(X_i)\bigg|\right] \leq \E_X\E_\eps\left[\sup_{f\in\calF_M}\bigg|\frac{1}{n}\sum_{i=1}^n \eps_if(X_i)\bigg|\right] + \delta
	.\]

	\textit{Step 2: Deal with the term that is conditional on \(\{X_i\}\).} Let \(\calG_\delta = \{g_1,\dots,g_{K(\delta)}\}\) be such that for every \(f\in\calF_M\) there is a  \(g \in \calG_\delta\) such that \(\left\|f-g\right\|_{L_1(\P_n)}<\delta\). Since \(\log\calN\left(\delta,\calF_m,L_1(\P_n)\right)=o_p(n)\), we know that it is possible to pick a \(\calG_\delta\) in this fashion with probability approaching 1. Note that:
	\begin{itemize}
		\item Cardinality of \(\calG_\delta\): \(\left|\calG_\delta\right| = \calN\left(\delta,\calF_M,\|\cdot\|_{L_1(\P_n)}\right).\)
		\item Envelope of \(\calG_\delta\): by construction \(\calF_M \leq M\) so we can assume that \(\calG_\delta\leq M\).
	\end{itemize}
	Then, for all \(f\in\calF_M\) we have that, for some  \(g\in\calG_\delta\):
	\begin{align*}
		\bigg|\frac{1}{n}\sum_{i=1}^n \eps_if(X_i)\bigg|
		&\leq \bigg|\frac{1}{n}\sum_{i=1}^n \eps_ig(X_i)\bigg| + \bigg|\frac{1}{n}\sum_{i=1}^n \eps_i\left(f(X_i) - g(X_i)\right)\bigg| \\
		&\leq \bigg|\frac{1}{n}\sum_{i=1}^n \eps_ig(X_i)\bigg|+\delta 
	\end{align*}
	This gives us that
	\[
		\E_\eps\left[\sup_{f\in\calF_M}\bigg|\frac{1}{n}\sum_{i=1}^n \eps_if(X_i)\bigg|\right] \leq \E_\eps\left[\sup_{g\in\calG_\delta}\bigg|\frac{1}{n}\sum_{i=1}^n \eps_ig(X_i)\bigg|\right]+\delta
	.\]
	\textit{Step 3: Apply the Maximal Inequality}. We bound the first term in the last display using the maximal inequality in Lemma~\ref{lemma:vdv2.2.2}, for the particular case of the \(\psi_2\) Orlicz norm \footnote{We know that, in any finite sample, this Orlicz norm exists because our empirical expectation is bounded.}: if \(D = \{f_1,\dots,f_M\}\) then 
	\[\E\left[\sup_{f\in D}\bigg|f(X_i)\bigg|\right] \leq C\sqrt{1+\log m}\]
	for any \(C\) with  \(\E\left[\exp\left(\frac{f(X_i)}{C^2}\right)-1\right]\leq 1\) for all \(f\in D\). In our setting we will apply this to the functions \(\frac{1}{n}\sum_{i=1}^n \eps_i g(X_i)\) for \(g\in\calG_\delta\), with the \(g(X_i)\) treated as fixed so that  these are considered random variables in \(\eps_i\). In our setting we can bound:
	\[
		\E_\eps\left[\sup_{g\in\calG_\delta}\bigg|\frac{1}{n}\sum_{i=1}^n \eps_ig(X_i)\bigg|\right] \leq C_\delta \sqrt{1 + \log\calN\left(\delta,\calF_M,\|\cdot\|_{L_1(\P_n)}\right)}
	.\] 
	for such a \(C_\delta\) such that, for all \(g \in \calG_\delta\)
	 \[
		 \E_\eps\left[\exp\left(\bigg(\frac{1}{n}\sum_{i=1}^n \eps_ig(X_i)\bigg)^2/C_\delta^2\right)-1\right]\leq 1
	.\] 
	Hoeffding's inequality (Lemma~\ref{lemma:hoeffding}) gives us that, for a general Rademachar process:
	\begin{align*}
		\P_\eps\left(\bigg|\sum_{i=1}^n \eps_ia_i\bigg|>x\right) &\leq 2\exp\left(-\frac{1}{2}\frac{x^2}{\|a\|^2}\right)\\
		\intertext{Where the norm above is the standard Euclidean norm. In our setting:}
		\P_\eps\left(\bigg|\sum_{i=1}^n \eps_i\frac{g(X_i)}{n} \bigg| > x\right) &\leq 2\exp\left(-\frac{1}{2}\frac{x^2}{n^{-2}\sum g(X_i)^2} \right)\\
		\intertext{As discussed above, we can uniformly bound  \(\calG_\delta\) by  \(M\). The exponential is negative so it is decreasing in the numerator and increasing in the denominator. This allows us to get:} 
		\P_\eps\left(\bigg|\sum_{i=1}^n \eps_i\frac{g(X_i)}{n} \bigg| > x\right) &\leq 2\exp\left(-\frac{n x^2}{2M^2} \right)
	\end{align*}
	Now apply Lemma~\ref{lemma:vdv2.2.1}. If \(\P\left(|X|>x\right)\leq Ke^{-Dx^2}\) then the \(\psi_2\)-Orlicz norm of \(X\) is less than  \(\sqrt{(1+K)/D}\). Using the above take \(K=2\) and \(D = n/(2M^2)\) to get \(C_\delta = \sqrt{6M^2/n}\). Putting this together with the original application of the maximal inequality towards the top of Step 3, we get:
	\begin{align*}
		\E_\eps\left[\sup_{g\in\calG_\delta}\bigg|\frac{1}{n}\sum_{i=1}^n \eps_ig(X_i)\bigg|\right]&\leq \sqrt{6M^2/n + \log\calN\left(\delta,\calF_M,\|\cdot\|_{L_1(\P_n)}\right)/n}
	\end{align*}
	By assumption \(\log\calN(\cdot)/n = o_P(1)\) and the first term under the square root is \(o(1)\) so that this whole thing is \(o_p(1)\). All together, combining this with the result of Step 2, we have shown that, for any \(\delta > 0\):
	\[
		\E_\eps\left[\sup_{f\in\calF_M}\bigg|\frac{1}{n}\sum_{i=1}^n \eps_if(X_i)\bigg|\right] \leq \delta + o_p(1) 
	.\] 
	So that the whole thing (\(\E_\eps[\sup_{f\in\calF_M}\dots]\)) is \(o_p(1)\).

	\textit{Step 4: Put all the parts together.}
	

\end{proof}



