\section{Empirical Processes}

These notes follow Section 2 in VdV\&W. So far, we have discussed theory for \(X_n \overset{L}{\to} X\) where both \(X_n\) and  \(X\) are random elements in  \(\ell^\infty(T)\). The classic example that we have kept in mind is convergence of the empirical CDF process, \(X_n(t) = \frac{1}{\sqrt{n}}\sum_{i=1}^n \left(\mathds{1}\{X_i \leq t\} - \P(X\leq t)\right)\). In this next section we will build on the theory developed to show the convergence of some empirical processes on \(\ell^\infty\).

\begin{definition}[Empirical Measure]
	\label{def:empirical-measure}
	For a random sample \(\{X_i\}_{i=1}^n\), the empirical measure \(\P_n\) is the measure constructed from the sample (putting mass \(1/n\) at each  \(X_i\)). That is, for any set \(C\):
	 \[
		 \P_n(C) := \frac{1}{n}\sum_{i=1}^n \mathds{1}\{X_i \in C\}  
	.\]
	We can also write this in terms of the degenerate measures on each \(X_i\):
	 \[
		 \P_n := \frac{1}{n}\sum_{i=1}^n \delta_{X_i} 
	.\] 
\end{definition}

\begin{definition}[Empirical Process]
	\label{def:empirical-process}
	For a random sample \(\{X_i\}_{i=1}^n\) drawn from common distribution \(P\), the empirical process \(\mathbb{G}_n\) is the scaled and demeaned measure on \(X\) given by:
	\[\mathbb{G}_n(C) := \frac{1}{\sqrt{n}}\sum_{i=1}^n \left(\mathds{1}\{X_i \in C\} - P(X_i \in C)\right).\]
	This is often related to the empirical measure in Definition~\ref{def:empirical-measure} by
	\[
		\mathbb{G}_n = \sqrt{n}\left(\P_n - P\right)
	.\]
	Or written in terms of the degenerate measures on each \(X_i\):
	 \[
		 \mathbb{G}_n = \frac{1}{\sqrt{n}}\sum_{i=1}^n \left(\delta_{X_i}-P\right) 
	.\] 
\end{definition}

\begin{remark}[Notation]
    \label{rem:ep-notation}
	We will make the following notations to save space later on. For a measure \(\mathbb{Q}\) on a space let \(\mathbb{Q}f = \E_{\mathbb{Q}}[f(X)]\). In the above \(\P_nf = \E_{n}[f(X)] = \frac{1}{n}\sum_{i=1}^n f(X_i) \) and \(\mathbb{G}_n f = \frac{1}{\sqrt{n}}\sum_{i=1}^n \left(f(X_i) - Pf\right)\).

	With this notation:
	\begin{align*}
		\P_nf\overset{\text{a.s}}{\longrightarrow}Pf&\hbox{  }\text{ is just saying }\hbox{ }\frac{1}{n}\sum_{i=1}^n f(X_i)\overset{\text{a.s}}{\longrightarrow}\E\left[f(X)\right]\\ 
		\mathbb{G}_nf\overset{L}{\longrightarrow}N(0,\sigma^2)&\hbox{  }\text{ is just saying }\hbox{  }\frac{1}{\sqrt{n}}\sum_{i=1}^n \left(f(X_i)-\E\left[f(X)\right]\right)\overset{L}{\longrightarrow}N(0,\sigma^2) 
	\end{align*}
	By LLN and CLT we have that for any function \(f\),  \(\P_nf \to_{a.s}Pf\) and \(\mathbb{G}_nf \overset{L}{\to}N\left(0,P\left(f-Pf\right)^2\right)\)
\end{remark}

\begin{example}[Classes of Functions]
	\label{ex:calF}
	LLN and CLT establish the behavior of the empirical measure \(\P_nf\) and the empirical process \(\mathbb{G}_nf\) for a fixed function \(f\) (which could even be vector valued). However, we often want to study the behavior of the empirical measure of empirical process over a class of functions  \(\calF\). In this case we can think of  \(\mathbb{G}_n(\calF)\) or \(\P_n(\calF)\) as random maps onto \(\ell^\infty(\calF)\). The marginal, \(\mathbb{G}_nf\) or \(\P_nf\), is then the behavior of the empirical measure/process for a single function \(f\in\calF\).

	Mapping this back to the empirical CDF example of before let \(\calF = \left\{f_t:\SR\to \SR\mid f_t(x) = \mathds{1}\{x\leq t\}, t \in T\right\}\). Before, we considered convergence of the whole CDF through the map \(X_n:\Omega_n \to \ell^\infty(T)\) with the marginals \(X_n(t) = \frac{1}{n}\sum_{i=1}^n \mathds{1}\{X_i \leq t\}  \). With these new definitions/notations, we equivalently consider convergence of the entire CDF through the map \(\P_n(\calF):\Omega_n \to \ell^\infty(\calF)\) with marginals \(\P_nf_t = \frac{1}{n}\sum_{i=1}^n \mathds{1}\{X_i \leq t\} \).

	This sort of notation/generality is useful as we can consider the behavior of the empirical measure or empirical process over a larger class of functions. For example, if we wanted to study an entire semiparametric model we may consider the behavior of \(\mathbb{G}_n(\calF)\) where 
	\[
		\calF = \left\{f(x;\theta)\text{ for some }\theta\in\Theta\right\}
	.\]
	Or, if we wanted to consider convergence after imposing some shape restriction, we may take
	\[
		\calF = \left\{f:X\to\SR\mid f\text{ is monotonic}\right\}
	.\] 
\end{example}

\begin{remark}[Notation]
	Sometimes we use \(\wcov\) to denote weak convergence/convergence in law instead of  \(\overset{L}{\to}\).	
\end{remark}

\begin{remark}[Definition of \(\ell^\infty\) Space]
    \label{rem:def-recall}
	It is useful to review the \(\ell^\infty(T)\) space for an arbitrary index space  \(T\). Define:
	\begin{equation}
		\label{eq:ell-infty}
		\ell^\infty(T) = \left\{f:T\to\SR: \sup_{t\in T}\left|f(t)\right| <\infty\right\}
	\end{equation}
	and equip this space with the sup-norm, \(\left\|f\right\|_T = \sup_{t\in T}\left|f(t)\right|\). Note that, for any \(\calF\),  \(\mathbb{G}_n(\calF)\) can be viewed as a random map into \(\ell^\infty(\calF)\) for each \(n\). Boundedness comes from the finiteness of the sample. We will sometimes make the notation \(\left\|\mathbb{Q}\right\|_\calF = \sup_{f\in\calF}\left|\mathbb{Q}f\right|\) for a given measure \(\mathbb{Q}\).
\end{remark}

Now make some important definitions and then talk about how they relate to what we want to show. 

\begin{definition}[Glivenko-Cantelli Class]
	\label{def:gc-class}
	A class of functions, \(\calF\), for which
	\begin{equation}
		\label{eq:gc-def}
		\left\|\P_n-P\right\|_\calF \to_p 0
	\end{equation}
	is called a Glivenko-Cantelli class, or a \(P\)-Glivenko-Cantelli class to emphasize the dependence on the underlying measure \(P\) from which the sample is drawn.
\end{definition}

\begin{definition}[Donsker Class]
	\label{def:donsker-class}
	A class of functions, \(\calF\), for which
	 \begin{equation}
		\label{eq:donsker-def}
		\mathbb{G}_n(\calF) \overset{L}{\longrightarrow} \mathbb{G}(\calF)
	\end{equation}
	where \(\mathbb{G}\) is a tight, Borel measurable element in \(\ell^\infty(\calF)\), is called a Donsker class, or  \(P\)-Donsker class to emphasize the dependence on the underlying measure  \(P\) from which the sample is drawn.
\end{definition}

A Donsker class is trivially Glivenko-Cantelli. 

\begin{example}[Some Donsker Classes]
	Some examples of function classes:
	\begin{enumerate}
		\item If \(\calF\) consists of a single function with finite variance then \(\calF\) is Donsker by the Central Limit Theorem. That is  \(\mathbb{G}_n \overset{L}{\to}\mathbb{G}\) where \(\mathbb{G}\) is a tight element on \(\ell^\infty(\calF) = \ell^\infty(\{f\} )\) 
		\item The class of functions \(\calF = \left\{f(x) = x'\beta: \beta \in \calB\right\} \) is Donsker if \(\calB\) is bounded. 
		\item The class of monotonic densities on \([0,1]\) is Donsker. 
		\item The class of square integrable functions is not Donsker (too large).
	\end{enumerate}	
\end{example}

How do we know if \(\mathbb{G}_N \wcov \mathbb{G}\) where \(\mathbb{G}\) is a tight, Borel measurable element on \(\ell^\infty(\calF)\)? By Theorem~\ref{thm:vdv1.5.4} we know that \(X_n\) weakly converges if and only if \(X_n\) is asymptotically tight and the marginals  \(\left(X_n(t_1),\dots,X_n(t_k)\right)\) converge weakly to a limit for every finite subset. Moreover, by Lemma~\ref{lemma:vdv1.5.1} asymptotic measurability of the process is equivalent to asymptotic measurability of the marginals. By the Central Limit Theorem, we typically have weak convergence and asymptotic measurability of the marginals, what remains is to show asymptotic tightness. 

Theorem~\ref{thm:vdv1.5.7} characterizes asymptotic tightness in terms of \(\rho\)-equicontinuity. Much of the work in showing tightness will be to find some semimetric \(\rho\) on \(\calF\) such that for any \(\eps, \eta > 0\) there is a  \(\delta > 0\) such that 
\begin{equation}
	\label{eq:rho-ec}
	\lim\sup_{n\to\infty}\P^\star\left(\sup_{\rho(f,g)<\delta}\left|\mathbb{G}_n(f)-\mathbb{G}_n(g)\right| > \eps\right) <\eta.
\end{equation}

A typical approach will be to let \(\calF_{{}\delta} = \left\{f,g \in \calF, \rho(f,g)<\delta\right\}\). If we can show that, for some \(M(\delta)\) that goes to 0 as \(\delta\downarrow 0\)
\begin{align*}
	\E\left[\left\|\mathbb{G}_n\right\|_{\calF_{{}\delta}}\right] 
	&= \E\left[\sup_{\rho(f,g)<\delta}\left|\mathbb{G}_n(f)-\mathbb{G}_n(g)\right|\right] \\
	&= \E\left[\sup_{\rho(f,g)<\delta}\left|\frac{1}{\sqrt{n}} \sum_{i=1}^n\left\{ f(X_i) - \E[f(X_i)] - g(X_i) + \E[g(X_i)]\right\}\right|\right] \\
	&\leq M(\delta)
\end{align*}
Then, we would get the result in \eqref{eq:rho-ec} by Markov's inequality. This type of result, that \(\E\left[\left\|\mathbb{G}_n\right\|_{\calF_{{}\delta}}\right] \leq M(\delta)\) is called a maximal inequality and is immensely useful.

Obtaining such a maximal inequality/establishing asymptotic tightness is dependent on the space not being ``too large" (loosely speaking). In the example above, the class \(\calF = \{f(x) = x'\beta\mid \beta\in\calB\} \) is Donsker so long as \(\calB\) is bounded. To illustrate, see in the single dimensional case that
\[
	\sup_{b\in \calB}\left|\frac{1}{\sqrt{n}}\sum_{i=1}^n x_ib - \E[xb] \right| = \sup_{b\in\calB} \left|\frac{1}{\sqrt{n}}\sum_{i=1}^n x_i - \E[x] \right||b|
.\]
If we don't impose \(|b|\leq M\) then this will blow up to \(+\infty\) with probability 1, whereas if we do we have that this is  \(O_p(1)\). For more involved function classes, we want a way of measuring whether  \(\calF\) is large or not. This motivates the definitions of bracketing and covering numbers below.

\begin{definition}[Covering Number]
	\label{def:covering}
	The covering number, \(\calN\left(\eps,\calF,\|\cdot\|\right)\) of a class of functions \(\calF\) is the smallest number of balls of radius  \(\eps\) under  \(\|\cdot\|\) needed to cover the set \(\calF\).
\end{definition}

\begin{definition}[Bracketing Number]
	\label{def:bracketing}
	Given two functions, \(\ell\) and \(u\), the bracket \([\ell,u]\) is the set of all functions  \(f\) with  \(\ell(x)\leq f\leq u(x)\) for all \(x\). An  \(\eps\)-bracket is a bracket  \([\ell,u]\) with  \(\|u-\ell\|< \eps\). The bracketing number \(\calN_{[]}\left(\eps,\calF,\|\cdot\|\right)\) is the minimum number of \(\eps\)-brackets needed to cover  \(\calF\).
\end{definition}

\begin{example}[Covering Numbers]
	\label{ex:covering}
	Let \(A = [0,1]\) and  \(\|\cdot\|\) be the standard Euclidean norm\footnote{If we want to view this as a function class we can equivalently say \(A\) is the set of constant functions taking values in the interval  \([0,1]\) and consider any \(\ell^p\) norm on this class}.
\end{example}
