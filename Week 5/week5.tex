\section{Empirical Processes}

These notes follow Section 2 in VdV\&W. So far, we have discussed theory for \(X_n \overset{L}{\to} X\) where both \(X_n\) and  \(X\) are random elements in  \(\ell^\infty(T)\). The classic example that we have kept in mind is convergence of the empirical CDF process, \(X_n(t) = \frac{1}{\sqrt{n}}\sum_{i=1}^n \left(\mathds{1}\{X_i \leq t\} - \P(X\leq t)\right)\). In this next section we will build on the theory developed to show the convergence of some empirical processes on \(\ell^\infty\).

\begin{definition}[Empirical Measure]
	\label{def:empirical-measure}
	For a random sample \(\{X_i\}_{i=1}^n\), the empirical measure \(\P_n\) is the measure constructed from the sample (putting mass \(1/n\) at each  \(X_i\)). That is, for any set \(C\):
	 \[
		 \P_n(C) := \frac{1}{n}\sum_{i=1}^n \mathds{1}\{X_i \in C\}  
	.\]
	We can also write this in terms of the degenerate measures on each \(X_i\):
	 \[
		 \P_n := \frac{1}{n}\sum_{i=1}^n \delta_{X_i} 
	.\] 
\end{definition}

\begin{definition}[Empirical Process]
	\label{def:empirical-process}
	For a random sample \(\{X_i\}_{i=1}^n\) drawn from common distribution \(P\), the empirical process \(\mathbb{G}_n\) is the scaled and demeaned measure on \(X\) given by:
	\[\mathbb{G}_n(C) := \frac{1}{\sqrt{n}}\sum_{i=1}^n \left(\mathds{1}\{X_i \in C\} - P(X_i \in C)\right).\]
	This is often related to the empirical measure in Definition~\ref{def:empirical-measure} by
	\[
		\mathbb{G}_n = \sqrt{n}\left(\P_n - P\right)
	.\]
	Or written in terms of the degenerate measures on each \(X_i\):
	 \[
		 \mathbb{G}_n = \frac{1}{\sqrt{n}}\sum_{i=1}^n \left(\delta_{X_i}-P\right) 
	.\] 
\end{definition}

\begin{remark*}[Notation]
    \label{rem:ep-notation}
	We will make the following notations to save space later on. For a measure \(\mathbb{Q}\) on a space let \(\mathbb{Q}f = \E_{\mathbb{Q}}[f(X)]\). E.j: \(\P_nf = \E_{n}[f(X)] = \frac{1}{n}\sum_{i=1}^n f(X_i) \) and \(\mathbb{G}_n f = \frac{1}{\sqrt{n}}\sum_{i=1}^n \left(f(X_i) - Pf\right)\).

	With this notation:
	\begin{align*}
		\P_nf\overset{\text{a.s}}{\longrightarrow}Pf&\hbox{  }\text{ is just saying }\hbox{ }\frac{1}{n}\sum_{i=1}^n f(X_i)\overset{\text{a.s}}{\longrightarrow}\E\left[f(X)\right]\\ 
		\mathbb{G}_nf\overset{L}{\longrightarrow}N(0,\sigma^2)&\hbox{  }\text{ is just saying }\hbox{  }\frac{1}{\sqrt{n}}\sum_{i=1}^n \left(f(X_i)-\E\left[f(X)\right]\right)\overset{L}{\longrightarrow}N(0,\sigma^2) 
	\end{align*}
	By LLN and CLT we have that for any function \(f\),  \(\P_nf \to_{a.s}Pf\) and \(\mathbb{G}_nf \overset{L}{\to}N\left(0,P\left(f-Pf\right)^2\right)\)
\end{remark*}

\begin{example}[Classes of Functions]
	\label{ex:calF}
	LLN and CLT establish the behavior of the empirical measure \(\P_nf\) and the empirical process \(\mathbb{G}_nf\) for a fixed function \(f\) (which could even be vector valued). However, we often want to study the behavior of the empirical measure of empirical process over a class of functions  \(\calF\). In this case we can think of  \(\mathbb{G}_n(\calF)\) or \(\P_n(\calF)\) as random maps onto \(\ell^\infty(\calF)\). The marginal, \(\mathbb{G}_nf\) or \(\P_nf\), is then the behavior of the empirical measure/process for a single function \(f\in\calF\).

	Mapping this back to the empirical CDF example of before let \(\calF = \left\{f_t:\SR\to \SR\mid f_t(x) = \mathds{1}\{x\leq t\}, t \in T\right\}\). Before, we considered convergence of the whole CDF through the map \(X_n:\Omega_n \to \ell^\infty(T)\) with the marginals \(X_n(t) = \frac{1}{n}\sum_{i=1}^n \mathds{1}\{X_i \leq t\}  \). With these new definitions/notations, we equivalently consider convergence of the entire CDF through the map \(\P_n(\calF):\Omega_n \to \ell^\infty(\calF)\) with marginals \(\P_nf_t = \frac{1}{n}\sum_{i=1}^n \mathds{1}\{X_i \leq t\} \).

	This sort of notation/generality is useful as we can consider the behavior of the empirical measure or empirical process over a larger class of functions. For example, if we wanted to study an entire semiparametric model we may consider the behavior of \(\mathbb{G}_n(\calF)\) where 
	\[
		\calF = \left\{f(x;\theta)\text{ for some }\theta\in\Theta\right\}
	.\]
	Or, if we wanted to consider convergence after imposing some shape restriction, we may take
	\[
		\calF = \left\{f:X\to\SR\mid f\text{ is monotonic}\right\}
	.\] 
\end{example}

\begin{remark*}[Notation]
	Sometimes we use \(\wcov\) to denote weak convergence/convergence in law instead of  \(\overset{L}{\to}\).	
\end{remark*}

\begin{remark*}[Definition of \(\ell^\infty\) Space]
    \label{rem:def-recall}
	It is useful to review the \(\ell^\infty(T)\) space for an arbitrary index space  \(T\). Define:
	\begin{equation}
		\label{eq:ell-infty}
		\ell^\infty(T) = \left\{f:T\to\SR: \sup_{t\in T}\left|f(t)\right| <\infty\right\}
	\end{equation}
	and equip this space with the sup-norm, \(\left\|f\right\|_T = \sup_{t\in T}\left|f(t)\right|\). Note that, for any \(\calF\),  \(\mathbb{G}_n(\calF)\) can be viewed as a random map into \(\ell^\infty(\calF)\) for each \(n\). Boundedness comes from the finiteness of the sample. We will sometimes make the notation \(\left\|\mathbb{Q}\right\|_\calF = \sup_{f\in\calF}\left|\mathbb{Q}f\right|\) for a given measure \(\mathbb{Q}\).
\end{remark*}

Now make some important definitions and then talk about how they relate to what we want to show. 

\begin{definition}[Glivenko-Cantelli Class]
	\label{def:gc-class}
	A class of functions, \(\calF\), for which
	\begin{equation}
		\label{eq:gc-def}
		\left\|\P_n-P\right\|_\calF \to_p 0
	\end{equation}
	is called a Glivenko-Cantelli class, or a \(P\)-Glivenko-Cantelli class to emphasize the dependence on the underlying measure \(P\) from which the sample is drawn.
\end{definition}

\begin{definition}[Donsker Class]
	\label{def:donsker-class}
	A class of functions, \(\calF\), for which
	 \begin{equation}
		\label{eq:donsker-def}
		\mathbb{G}_n(\calF) \overset{L}{\longrightarrow} \mathbb{G}(\calF)
	\end{equation}
	where \(\mathbb{G}\) is a tight, Borel measurable element in \(\ell^\infty(\calF)\), is called a Donsker class, or  \(P\)-Donsker class to emphasize the dependence on the underlying measure  \(P\) from which the sample is drawn.
\end{definition}

A Donsker class is trivially Glivenko-Cantelli. 

\begin{example}[Some Donsker Classes]
	Some examples of function classes:
	\begin{enumerate}
		\item If \(\calF\) consists of a single function with finite variance then \(\calF\) is Donsker by the Central Limit Theorem. That is  \(\mathbb{G}_n \overset{L}{\to}\mathbb{G}\) where \(\mathbb{G}\) is a tight element on \(\ell^\infty(\calF) = \ell^\infty(\{f\} )\) 
		\item The class of functions \(\calF = \left\{f(x) = x'\beta: \beta \in \calB\right\} \) is Donsker if \(\calB\) is bounded. 
		\item The class of monotonic densities on \([0,1]\) is Donsker. 
		\item The class of square integrable functions is not Donsker (too large).
	\end{enumerate}	
\end{example}

How do we know if \(\mathbb{G}_N \wcov \mathbb{G}\) where \(\mathbb{G}\) is a tight, Borel measurable element on \(\ell^\infty(\calF)\)? By Theorem~\ref{thm:vdv1.5.4} we know that \(X_n\) weakly converges if and only if \(X_n\) is asymptotically tight and the marginals  \(\left(X_n(t_1),\dots,X_n(t_k)\right)\) converge weakly to a limit for every finite subset. Moreover, by Lemma~\ref{lemma:vdv1.5.1} asymptotic measurability of the process is equivalent to asymptotic measurability of the marginals. By the Central Limit Theorem, we typically have weak convergence and asymptotic measurability of the marginals, what remains is to show asymptotic tightness. 

Theorem~\ref{thm:vdv1.5.7} characterizes asymptotic tightness in terms of \(\rho\)-equicontinuity. Much of the work in showing tightness will be to find some semimetric \(\rho\) on \(\calF\) such that for any \(\eps, \eta > 0\) there is a  \(\delta > 0\) such that 
\begin{equation}
	\label{eq:rho-ec}
	\lim\sup_{n\to\infty}\P^\star\left(\sup_{\rho(f,g)<\delta}\left|\mathbb{G}_n(f)-\mathbb{G}_n(g)\right| > \eps\right) <\eta.
\end{equation}

A typical approach will be to let \(\calF_{{}\delta} = \left\{f,g \in \calF, \rho(f,g)<\delta\right\}\). If we can show that, for some \(M(\delta)\) that goes to 0 as \(\delta\downarrow 0\)
\begin{align*}
	\E\left[\left\|\mathbb{G}_n\right\|_{\calF_{{}\delta}}\right] 
	&= \E\left[\sup_{\rho(f,g)<\delta}\left|\mathbb{G}_n(f)-\mathbb{G}_n(g)\right|\right] \\
	&= \E\left[\sup_{\rho(f,g)<\delta}\left|\frac{1}{\sqrt{n}} \sum_{i=1}^n\left\{ f(X_i) - \E[f(X_i)] - g(X_i) + \E[g(X_i)]\right\}\right|\right] \\
	&\leq M(\delta)
\end{align*}
Then, we would get the result in \eqref{eq:rho-ec} by Markov's inequality. This type of result, that \(\E\left[\left\|\mathbb{G}_n\right\|_{\calF_{{}\delta}}\right] \leq M(\delta)\) is called a maximal inequality and is immensely useful.

Obtaining such a maximal inequality/establishing asymptotic tightness is dependent on the space not being ``too large" (loosely speaking). In the example above, the class \(\calF = \{f(x) = x'\beta\mid \beta\in\calB\} \) is Donsker so long as \(\calB\) is bounded. To illustrate, see in the single dimensional case that
\[
	\sup_{b\in \calB}\left|\frac{1}{\sqrt{n}}\sum_{i=1}^n x_ib - \E[xb] \right| = \sup_{b\in\calB} \left|\frac{1}{\sqrt{n}}\sum_{i=1}^n x_i - \E[x] \right||b|
.\]
If we don't impose \(|b|\leq M\) then this will blow up to \(+\infty\) with probability 1, whereas if we do we have that this is  \(O_p(1)\). For more involved function classes, we want a way of measuring whether  \(\calF\) is large or not. This motivates the definitions of bracketing and covering numbers below.

\begin{definition}[Covering Number]
	\label{def:covering}
	The covering number, \(\calN\left(\eps,\calF,\|\cdot\|\right)\) of a class of functions \(\calF\) is the smallest number of balls of radius  \(\eps\) under  \(\|\cdot\|\) needed to cover the set \(\calF\).
\end{definition}

\begin{definition}[Bracketing Number]
	\label{def:bracketing}
	Given two functions, \(\ell\) and \(u\), the bracket \([\ell,u]\) is the set of all functions  \(f\) with  \(\ell(x)\leq f\leq u(x)\) for all \(x\). An  \(\eps\)-bracket is a bracket  \([\ell,u]\) with  \(\|u-\ell\|< \eps\). The bracketing number \(\calN_{[\hspace{0.1em}]}\left(\eps,\calF,\|\cdot\|\right)\) is the minimum number of \(\eps\)-brackets needed to cover  \(\calF\).
\end{definition}

\begin{example}[Covering Number]
	\label{ex:covering}
	Let \(A = [0,1]\) and  \(\|\cdot\|\) be the standard Euclidean norm\footnote{If we want to view this as a function class we can equivalently say \(A\) is the set of constant functions taking values in the interval  \([0,1]\) and consider any \(L_p\) norm on this class}.
	\begin{enumerate}
		\item If \(\eps \geq 1/2\), then a ball centered at  \(1/2\) covers the entire interval so  \(\calN\left(\eps,A,\left|\cdot\right|\right) = 1\).
		\item If \(\eps < 1/2\), then we need \(\lceil \frac{1}{2\eps}\rceil\) balls to cover \(A\).
	\end{enumerate}
	Note that (i) in this example the covering number coincides with the bracketing number (ii) in general the balls needed to cover \(\calF\) need not be centered at points in \(\calF\) (iii) (in general) as \(\eps\downarrow 0\) we have that  \(\calN(\eps,\calF,\|\cdot\|)\uparrow\infty\). 
\end{example}

\begin{example}[Bracketing Number]
	\label{ex:brackering}
	Suppose \(x\) takes values in  \([0,1]\) and let \(\calF = \left\{f(x)=x\beta,\text{ for }\beta \in [0,1]\right\}\). Then, if \(\beta_i < \beta_{i+1}\),  \([x\beta_i, x\beta_{i+1}]\) forms a bracket containing all functions  \(f(x)=x\beta\) with  \(\beta_i \leq \beta\leq \beta_{i+1}\). Further note that \[
		\left\|x\beta_i - x\beta_{i+1}\right\| = \sup_{x\in[0,1]}|x||\beta_i-\beta_{i+1}| = |\beta_i -\beta_{i+1}|
	.\] 
	For any \(\eps>0\) break up  \([0,1]\) into  \([0,\eps,2\eps,\dots]\) and take \(\beta_i = (i-1)\eps\) to get brackets  \([x\beta_i,x\beta_{i+1}]\) of size \(\eps\). We need  \(\lceil 1/\eps\rceil\) of these brackets to cover  \(\calF\) so that  \(\calN_{[]}\left(\eps,\calF,\|\cdot\|_\infty\right)\leq \lceil 1/\eps\rceil < 2/\eps\).
\end{example}

\begin{remark*}[Bracketing vs. Covering Numbers]
    \label{rem:bracketing-covering}
	In general we have that \(\calN(\eps,\calF,\|\cdot\|)\leq \calN_{[]}(2\eps,\calF,\|\cdot\|)\), but no opposite relationship. This shows that bracketing numbers are in general stronger than covering numbers and give you better control over the class of functions. 

	We will see conditions for Glivenko-Cantelli and Donsker properties under both, but in general proving Glivenko-Cantelli involves using bracketing numbers whereas proving Donsker involves using covering numbers.
\end{remark*}


In general, finding the covering/bracketing number will be difficult but we will learn some tips. Verifying that a set is Donsker will often come down to showing that the covering/bracketing number does not go to infinity ``too fast." 

\subsection{Maximal Inequality}%
\label{subsec:maximal}

For an arbitrary set of functions, \(\calF\), want to develop an inequality that looks something like:
\[
	\E\left[\sup_{f\in\calF}\left|\frac{1}{\sqrt{n}}\sum_{i=1}^n \left(f(x_i)-\E[f(x)]\right) \right|\right]\leq \text{size}\left(\calF\right)
.\]
Or, rewriting in the notation of above:
\[
	\E\left[\sup_{f\in\calF}\left|\mathbb{G}_nf\right|\right]\leq \text{size}\left(\calF\right)
.\] 
This sort of inequality is useful as it can be used to show the uniform law of large numbers:
\[
	\E\left[\sup_{f\in\calF}\left|\left(\P_n - P\right)f\right|\right] \leq \frac{1}{\sqrt{n}}\text{size}\left(\calF\right)\hbox{  }\text{ + }\hbox{  }\text{Markov's Inequality}
.\] 
Or show asymptotic tightness through stochastic equicontinuity:
\[
	\E\left[\sup_{\rho(f,g)<\delta}\left|\mathbb{G}_n\left(f-g\right)\right|\right] \leq \text{size}\left(\calF_\delta\right)\hbox{  }\text{ + }\hbox{  }\text{Theorem~\ref{thm:vdv1.5.7}}
.\]
However, often we may need to change the exact application of these maximal inequalities. We will work out where these come from as we go along. The inequality will be presented for general stochastic processes (for our purposes, a stochastic process is a random map into \(\ell^\infty(T)\)). To build the maximal inequality, we will need to define a new norm which generalizes the \(L_p\) norms. We do so quickly below.

\subsubsection{Orlicz Norm}

\begin{definition}[Orlicz Norm]
	\label{def:orlicz}
	Let \(\psi\) be a non-decreasing, convex function with  \(\psi(0)=0\) and  \(X\) a random variable. Then, the Orlicz norm  \(\left\|X\right\|_\psi\) is defined as 
	\begin{equation}
		\label{eq:orlicz}
		\left\|X\right\|_\psi = \inf\left\{C > 0 : \E\psi\left(\frac{|X|}{C} \right)\leq 1\right\}
	\end{equation}
	Where here the infimum over the empty set is taken to be \(+\infty\).
\end{definition}
\begin{remark*}[Orlicz norms generalize \(L_p\)]
	Note that for any \(p\geq 1\) the function \(f(x) = x^p\) is convex and non-decreasing. With this in mind we can view the Orlicz norms as a generalization of the \(L_p\) norms to general convex and non-decreasing functions functions.
\end{remark*}

\begin{remark*}[Orlicz p-norms]
	Of particular interest will be the Orlicz norms generated by the functions
	\[
		\psi_p = e^{x^p}-1
	.\] 
	for \(p\geq 1\). The Orlicz norm in this case is often denoted \(\left\|\cdot\right\|_{\psi_p}\).  These norms give more weight to the tails of \(X\) than the standard  \(L_p\) norms. It is not the case that these norms are uniformly larger than all \(L_p\) norms, however, we do have the inequalities
	\begin{align*}
		\left\|X\right\|_{\psi_p} &\leq \left\|X\right\|_{\psi_q}\left(\log 2\right)^{p/q}\\ 
		\left\|X\right\|_p &\leq p!\left\|X\right\|_{\psi_1}
	\end{align*}
\end{remark*}

\begin{remark*}[Orlicz Norms and Markov's Inequality]
    \label{rem:orlicz}
	Any Orlicz norm can be used to bound tail probabilities. Using Markov's inequality:
	\[
		\P\left(\left|X\right|>x\right)\leq \P\left(\psi\left(|X|/\left\|X\right\|_\psi\right)\geq \psi\left(x/\left\|X\right\|_\psi\right)\right) \leq \frac{1}{\psi\left(x/\|X\|_\psi\right)} 
	.\] 
	For \(\psi_p(x) = e^{x^p}-1\) this leads to tail estimates like  \(\exp\left(-Cx^p\right)\) for any random variable with a finite \(\psi_p\)-norm. Conversely, an exponential tail bound of this type shows that  \(\left\|X\right\|_{\psi_p}\) is finite.
\end{remark*}

\begin{lemma}[Lemma 2.2.1 VdV\&W]
	\label{lemma:vdv2.2.1}
	Let \(X\) be a random variable  with  \(\P\left(|X|>x\right)\leq Ke^{-Dx^p}\) for every \(x\) and some (fixed) constants  \(K\) and  \(D\) and for some  \(p \geq 1\). Then, the Orlicz norm of \(X\) satisfies  \[\|X\|_{\psi_p}\leq \left((1+K)/D\right)^{1/p}\]
	In particular, this will mean that for \(C = \left((1+K)/D\right)^{1/p}\)
	\[
	    \E\left[\psi\left(\frac{|X|}{C}\right)\right]\leq 1
	.\] 
\end{lemma}
\begin{proof}
	By Fundamental Theorem of Calculus and Tonelli's Theorem, for any constant \(B\):
	\begin{align*}
		\E\left[e^{B|X|^p}-1\right]= \E\int_0^{|X|^p}Be^{Bs}\,ds = \int_0^\infty \P\left(|X> s^{1/p}\right)Be^{Bs}\,ds
	\end{align*}
	Now use the inequality on the tails of \(|X|\), plug in \(B = C^{-p}= D/(1+K)\), and see that the final equality is bounded by 1.
\end{proof}

Using the fact that \(\max|X_i|^P \leq \sum |X_i|^p\) we obtain for the \(L_p\) norms, the result that 
\[
	\left\|\max_{1\leq i\leq m}X_j\right\|_p = \left(\E\max_{1\leq i\leq m}\left|X_i\right|^p\right)^{1/p} \leq m^{1/p}\max_{1\leq i\leq m}\left\|X_i\right\|_p
.\]
We can generalize this for the Orlicz norm.
\begin{lemma}[Lemma 2.2.2 VdV\&W]
	\label{lemma:vdv2.2.2}
	Let \(\psi\) be a convex, non-decreasing, nonzero function with  \(\psi(0)=0\) and  \(\lim\sup_{x,y\to\infty}\psi(x)\psi(y)/\psi(cxy)<\infty\) for some constant \(c\). Then, for any random variables  \(X_1,\dots,X_m\),
	\begin{equation}
		\label{eq:vdv2.2.2}
		\left\|\max_{1\leq i\leq m}X_i\right\|_\psi \leq K\psi^{-1}(m)\max_{1\leq i\leq m}\left\|X_i\right\|_\psi
	\end{equation}
	For a constant \(K\) depending only on  \(\psi\).
\end{lemma}

\begin{proof}
	Without loss of generality, assume that  \(\psi(x)\psi(y)\leq \psi(cxy)\) for all \(x,y\geq 1\) and that \(\psi(1)\leq 1/2\).\footnote{If this is not the case there are constants \(\sigma \leq 1\) and \(\tau > 0\) such that  \(\phi(x)=\sigma\psi(\tau x)\) satisfies these conditions. Apply the inequality to  \(\phi\) and note that 
	 \[
		 \|X\|_\psi \leq  \|X\|_\phi/(\sigma\tau)\leq \|X\|_\psi/\sigma
	.\] }
	In this case, \(\psi(x/y)\leq \psi(cx)/\psi(y)\) for all \(x\geq y\geq 1\).\footnote{\(x/y \geq 1\) so \(\psi(x/y)\psi(y) \leq \psi\left(c(x/y)y\right)\)} Thus, for \(y\geq 1\) and any \(D\);
	\begin{align*}
	\max_{1\leq i\leq m}\psi\left(\frac{|X_i|}{Dy}\right) 
	&\leq \max_{1\leq i\leq m}\left[\frac{\psi(c|X_i|/D)}{\psi(y)} + \psi\left(\frac{|X_i|}{Dy}\right)\mathds{1}\left\{\frac{|X_i|}{Dy}<1\right\} \right] \\
	&\leq \sum_{i=1}^m \frac{\psi\left(c|X_i|D\right)}{\psi(y)} + \psi(1) 
	\end{align*}
	Let \(D = c\max_{1\leq i\leq m}\|X_i\|_\psi\), and take expectations to get:
	\[
	    \E\psi\left(\frac{\max|X_i|}{Dy}\right) \leq
		\frac{m}{\psi(y)} + \psi(1) 
	.\] 
	When \(\psi(1)\leq 1/2\) take \(y=\psi^{-1}(2m)\). Then: 
	\[
		\left\|\max_{1\leq i\leq m}\left|X_i\right|\right\|_\psi \leq \psi^{-1}(2m)c\max_{1\leq i\leq m}\left\|X_i\right\|_\psi
	.\] 
	By the convexity of \(\psi\) and the fact that  \(\psi(0)=0\), it follows that  \(\psi^{-1}(2m)\leq 2\psi^{-1}(m)\). This gives the result.
\end{proof}

To review, we have established the following inequalities above:
\begin{enumerate}
	\item For maximums of a finite number of random variables
	\[
		\E\left[\max_{1\leq i\leq m}\left|X_i\right|\right]\leq m \max_{1\leq i\leq m}\E\left[|X_i|\right]
	.\]
	\item Then, generalized this to the \(L_p\) norms 
	\[
		\left\|\max_{1\leq i\leq m}\left|X_i\right|\right\|_{L_p} \leq m^{1/p}\max_{1\leq i\leq m}\left\|X_i\right\|_{L_p}
	.\]
	\item Then, generalized this using the Orlicz norm (Definition~\ref{def:orlicz})
	\[
		\left\|\max_{1\leq i\leq m}|X_i|\right\|_\psi \leq K\psi^{-1}(m)\max_{1\leq i\leq m}\left\|X_i\right\|_\psi
	.\]
	In particular, taking \(\psi(a) = e^{a^2}-1\), we have that  \(\E\left[\max_{1\leq i\leq m}|X_i|\right] \leq C\sqrt{\log(m+1)}\) for any \(C\) such that  \(\max_{1\leq i\leq m}\E\left[\psi\left(\frac{|X_i|}{C}\right)\right]\leq 1\). Lemma~\ref{lemma:vdv2.2.1} gives a condition for the existence of such a \(C\).
\end{enumerate}


\subsection{Chaining and Inequalities for Infinite Classes}%

So far, we have developed inequalities that deal with finite number of random variables. These inequalities are useful for showing Donsker/Glivenko-Cantelli property for finite classes of functions, \(|\calF|<\infty\), just set \(X_i = \mathbb{G}_nf_i\). However, we often want to show uniform convergence for (uncountably) infinite classes of sets, \(|\calF| = |\SQ|\) or  \(|\calF|=|\SR|\). To do this, we will use a technique called \textit{chaining}. 

Roughly speaking, this will work whenever our class of functions \(\calF\) is ``separable", with respect to the empirical process \(\mathbb{G}_n\) (or empirical measure \(\P_n\)). This means there is a countable subset \(\tilde\calF\) of  \(\calF\)   such that \(\sup_\calF |\mathbb{G}_n(f)| = \sup_{\tilde\calF} |\mathbb{G}_n(f)|\). What does this buy us? If \(\tilde\calF_0\subset\tilde\calF_1\subset\tilde\calF_2 \dots \subset\tilde\calF\) is an infinite sequence of sets whose union is \(\tilde\calF\) and where each \(\tilde\calF_i\) is finite, then:
 \[
	 \lim_{k\to\infty} \sup_{\tilde\calF_k}|\mathbb{G}_n(f)| \overset{a.s}{=} \sup_{\tilde\calF} |\mathbb{G}_n(f)|\; \overset{\text{monotone convergence}}{\implies}\;\lim_{k\to\infty} \E\left[\sup_{\tilde\calF_k}|\mathbb{G}_n(f)|\right]=\E\left[\sup_{\tilde\calF}|\mathbb{G}_n(f)|\right]
.\] 
and by separability, the last expectation is equal to the expectation of the supremum over the whole class \(\calF\). To make this work, we want to make sure that we can apply the inequalities that we developed in the past section. Specifically, we want to make sure that the conditions of Lemma~ \ref{lemma:vdv2.2.1} hold. To do so, make a definition.

\begin{definition}[Subgaussian Process]
	\label{def:subgaussian}
	Let \(\mathbb{G}\) be a stochastic process on a space \(\calF\) equipped with a metric \(d(\cdot,\cdot)\). Then  \(\mathbb{G}\) is subgaussian if
	\begin{equation}
		\label{eq:subgaussian}
		\P\left(\left|\mathbb{G}(f)-\mathbb{G}(g)\right|> x\right)\leq 2e^{-1/2x^2/d^2(f,g)}
	\end{equation}
	for all \(f,g \in F\) and any  \(x\geq 0\).
\end{definition}

Also define a separable function as an analytic concept and then extend this to the case of stochastic processes.

\begin{definition}[Separable Function]
	\label{def:separable-function}
	A function \(f:A\to B\) from a topological space \(A\) into a topological space  \(B\) is separable if there is a countable, dense, subset  \(S \subset A\) such that  for any closed \(F \subset B\) and any open \(I \subset A\), if  \(f(t) \in F\) for all  \(t \in F \cap S\) then  \(f(t) \in F\) for all  \(t \in I\). This is often denoted as an \(S\)-separable function to emphasize the dependence on the countable, dense subset  \(S\).
\end{definition}
\begin{lemma}[Continuity and Separability]
	\label{lemma:continuous-separable}
	A continuous function \(f:\calX\to\calY\) from a separable space \(\calX\) onto  \(\calY\) is separable.
\end{lemma}


\begin{definition}[Separable Process; Shalizi 2007]
	\label{def:separable-process}
	A stochastic process on a topological space \(\calF\), \(\mathbb{G}(\cdot,\omega):\Omega \to \ell^\infty(\calF)\), is separable if there is a countable, dense, subset of \(\calF\), \(\tilde\calF\), and a measure zero set \(N\) such that for all \(\omega\not\in N\),  \(\mathbb{G}(\cdot,\omega)\) is \(\tilde\calF\)-separable.\footnote{Note that this requires a topology on \(\calF\). In the applications we will be talking about  \(\calF\) will be equipped with a metric  \(d\). This will generate a topology.} 
\end{definition}


Separability can be roughly interpreted as ensuring that the behavior of the function (and therefore the stochastic process) can be well described by its behavior on countable subset. This ensures some of the properties that we've seen above, namely that \(\sup_{f\in\tilde\calF}|\mathbb{G}(f)| \overset{a.s}{=} \sup_{f\in\calF}|\mathbb{G}(f)|\). We are now ready for the main theorem of this subsection, the proof of which will rely on the chaining argument roughly discussed above.

\begin{theorem}[Theorem 2.2.4 VdV\&W]
	\label{thm:vdv2.2.4}
	Let \(\mathbb{G}\) be a separable subgaussian process on a space \(\calF\) equipped with a metric  \(d(\cdot,\cdot)\) and let \(\text{\emph{diam}}(\calF) = \sup_{f,g\in\calF}d(f,g) \). Then
	\begin{align}
		\label{eq:vdv2.2.4-1}
		\E\sup_{f,g\in\calF}\left|\mathbb{G}(f)-\mathbb{G}(g)\right|
		&\leq K \int_{0}^{\text{\emph{diam}}(\calF)} \sqrt{\log \calN(\eps, \calF, d)}\;d\eps \\
		\label{eq:vdv2.2.4-2}
		\E\sup_{f\in\calF}\left|\mathbb{G}(f)\right| &\leq \E\left|\mathbb{G}(f_0)\right| + K \int_{0}^{\text{\emph{diam}}(\calF)} \sqrt{\log \calN(\eps, \calF, d)}\;d\eps,\hbox{ }\forall f_0\in\calF
	\end{align}
\end{theorem}

\begin{proof}
	Proof proceeds in steps. Let \(M = \text{diam}(\calF) = \sup_{f,g\in\calF}d(f,g)\). For any  \(f_0,f \in \calF\) we have that  \(d(f_0,g)\leq M\).	First step will be to build a ``chain" to almost any point in \(\calF\). Further, let \(\tilde\calF\) be the dense subset as described in Definitions~\ref{def:separable-function} and \ref{def:separable-process}.

	\emph{Step 1: Building a Chain.} Pick any \(f_0 \in\tilde\calF\) and let  \(\tilde\calF_0 = \{f_0\}.\) Build nesting sets, \(\calF_0 \subset\tilde\calF_1\subset\tilde\calF_2\subset\dots\subset\tilde\calF\), such that for each \(k\in\SN\)  \(\tilde\calF_k = \{f_1,\dots,f_{m(k)}\}\) is a maximal collection of points such that \(d(f_k,g_k) > \frac{M}{2^K} \) for any \(f_k,g_k\in\calF_k\). By definition of the packing numbers we know that  \(\calN\left(\frac{M}{2^{k+1}},\calF,d\right)\) balls cover \(\calF\). Putting a point at the center of each of these balls creates points that are at least distance \(\frac{M}{2^k}\) from each other. Similarly, if we could fit more points at least distance \(\frac{M}{2^{k}}\) distance away from each other than we could pack more balls of radius \(\frac{M}{2^{k+1}}\) into \(\calF\) by centering a ball at each point. So, \(|\tilde\calF_K| \leq \calN\left(\frac{M}{2^{k+1}},\calF,d
	\right)\) (Inequality comes because each \(\tilde\calF_k\) has to contain all previous sets). 

	Finally, link each point \(f_k \in \tilde\calF_k\) to a unique point  \(f_{k-1}\in\tilde\calF_{k-1}\) such that \(d(f_k,f_{k-1})\leq \frac{M}{2^{k-1}}\).\footnote{I found it helpful to remember here that \(\tilde\calF_{k-1}\subset\tilde\calF_k\). If no such \(f_{k-1}\) exists we could add  \(f_k\) to  \(\tilde\calF_{k-1}\), a contradiction. If  \(f_k \in \tilde\calF_{k-1}\) we can link it to itself. }

	\emph{Step 2: Use the chain to build a bound.} Using these links, for any \(f_k, g_k\in\tilde\calF_k\) we can build a chain back to \(f_0\):
	\begin{align*}
		\left|\mathbb{G}(f_k)-\mathbb{G}(g_k)\right|
		&=\left|\left(\mathbb{G}(f_k)-\mathbb{G}(f_0)\right) - \left(\mathbb{G}(g_k)-\mathbb{G}(f_0)\right)\right| \\
		&= \left|\sum_{j=0}^k\left(\mathbb{G}(f_i)-\mathbb{G}(f_{i-1})\right)-\sum_{j=0}^k \left(\mathbb{G}(g_i)-\mathbb{G}(g_{i-1})\right)\right|
	\end{align*}
	By the triangle inequality:
	\begin{equation}
		\label{eq:step2}
		\E\left[\max_{g_k,f_k\in\tilde\calF_k}\left|\mathbb{G}(f_k)-\mathbb{G}(g_k)\right|\right]
		\leq 2\sum_{j=0}^K \E\left[\max_{s_i\in\tilde\calF_i}\left|\mathbb{G}(s_i)-\mathbb{G}(s_{i-1})\right|\right]\tag{P-1}
	\end{equation}	
	With this setup, we can use the maximal inequalities developed above, applying them to the finite sets \(\tilde\calF_k\).

	\emph{Step 3: Try to control the jumps.} Recall that there are at most \(\calN\left(\frac{M}{2^{k+1}},\calF,d \right)\) points in \(\calF_k\) and that \(d(s_k,s_{k-1})\leq \frac{M}{2^{k-1}}\). By our maximal inequality in Lemma~\ref{lemma:vdv2.2.2}, taking \(\psi(a) = e^{a^2}-1\) we have that 
	\[
		\E\left[\max_{s_j\in\tilde\calF_j}\left|\mathbb{G}(s_j)-\mathbb{G}(s_{j-1})\right|\right] \leq C_j \sqrt{\log\left( \calN\left(\frac{M}{2^{j+1}},\calF,d\right)+1\right)}
	.\]
	For any constant \(C_j\) such that
	 \[
		 \E\left[\exp\left(\frac{\left(\mathbb{G}(s_j)-\mathbb{G}(s_{j-1})\right)^2}{c_j^2} \right)-1\right]\leq 1,\;\; \forall s_j\in\tilde\calF_j
	.\] 
	Since \(\mathbb{G}\) is subgaussian we know that \(\P\left(\left|\mathbb{G}(f)-\mathbb{G}(g)\right|>x\right)\leq 2e^{-\frac{1}{2}x^2/d^2(f,g) }\). By construction, we know that \(d(s_j,s_{j-1})\leq \frac{M}{2^{j-1}},\forall s_j\in\tilde\calF_j \). So
	\[
		\P\left(\left|\mathbb{G}(s_j)-\mathbb{G}(s_{j-1})\right|>x\right)\leq 2e^{-\frac{1}{2}\frac{x^2}{\lceil M/2^{j-1}\rceil^2}}
	.\] 
	By Lemma~\ref{lemma:vdv2.2.1} we can take \(C_j = \frac{\sqrt{3}M}{2^{j-1}}\) and combine with the other results in this section to get
	\begin{equation}
	\label{eq:step3}
	\E\left[\max_{s_j\in\tilde\calF_j}\left|\mathbb{G}(s_j)-\mathbb{G}(s_{j-1})\right|\right]\leq \frac{\sqrt{3}M}{2^{j-1}}\sqrt{\log\left(\calN\left(\frac{M}{2^{j+1}},\calF,d\right)+1\right)} \tag{P-2}
	\end{equation}
	
	\emph{Step 4: Combine Results of Previous Steps.} Combine the inequalities from \eqref{eq:step2} and \eqref{eq:step3} to get
	\begin{equation*}
		\E\left[\max_{g_k,f_k\in\tilde\calF_k}\left|\mathbb{G}(f_g)-\mathbb{G}(g_k)\right|\right]\leq \sqrt{12}M\sum_{j=0}^k \frac{1}{2^{j-1}}\sqrt{\log\left(\calN\left(\frac{M}{2^{j+1}},\calF,d \right)+1\right)} 
	\end{equation*}
	With some complex rearranging of squares, we can bound the sum in the display by it's integral up to a constant scale, dropping the added \(1\) in the log in the process.\footnote{Here we use the fact that \(\log(1+m)\leq 2\log(m)\) for \(m\geq 2\)} That is, we ultimately obtain for some constant \(K\): 
	\begin{equation}
		\label{eq:step4}
		\E\left[\max_{g_k,f_k\in\tilde\calF_k}\left|\mathbb{G}(f_g)-\mathbb{G}(g_k)\right|\right]\leq K \int_{0}^{M}   \sqrt{\log\left(\calN\left(\eps,\calF,d \right)\right)}\, d\eps \tag{P-3}
	\end{equation}

	\emph{Step 5: Conclude by Separability.} \(\left\{\tilde\calF_k\right\}_{k=1}^\infty\) is an increasing sequence of sets that approaches \(\tilde\calF\) and note that the bound in \eqref{eq:step4} does not depend on (little) \(k\). So, invoking monotone convergence and separability of \(\mathbb{G}\):
	\begin{align*}
		\E\left[\sup_{f,g\in\calF}\left|\mathbb{G}(f)-\mathbb{G}(g)\right|\right]
		&= \E\left[\sup_{f,g\in\tilde\calF}\left|\mathbb{G}(f)-\mathbb{G}(g)\right|\right] \\
		&= \lim_{k\to\infty}\E\left[\max_{f,g\in\tilde\calF}\left|\mathbb{G}(f)-\mathbb{G}(g)\right|\right] \\
		&\leq K \int_{0}^{M} \sqrt{\log\calN\left(\eps,\calF,d\right)}\,d\eps 
	\end{align*}
	This is the inequality in equation~\eqref{eq:vdv2.2.4-1}. To get equation~\eqref{eq:vdv2.2.4-2} fix any \(f_0\) and apply triangle inequality.
\end{proof}

\begin{remark*}[Comments on Theorem~\ref{thm:vdv2.2.4}]
    Theorem~\ref{thm:vdv2.2.4} is an involved result. Some remarks below.
	\begin{enumerate}
		\item We have shown that if \(\mathbb{G}_n\) is a separable subgaussian process then
			\[
				\E\left[\sup_{f,g\in\calF}\left|\mathbb{G}_n(f)-\mathbb{G}_n(g)\right|\right] \leq  K \int_{0}^{\text{diam}(\calF)}\sqrt{\log\calN\left(\eps,\calF,d\right)}\,d\eps 
			.\]
		Note that the right hand side does not depend on \(\mathbb{G}_n\) at all! Only on the ``size" of \(\calF\).
		\item So, suppose we want to show that \(\mathbb{G}_n\) is an asymptotically tight process on \(\calF\). By Theorem~\ref{thm:vdv1.5.7} it is sufficient (and necessary) to show that for every \(\eps,\eta >0\) there is a  \(\delta > 0\) such that: 
			\[
				\lim\sup_{n\to\infty}\P\left(\sup_{\rho(f,g)\leq \delta}\left|\mathbb{G}_n(f)-\mathbb{G}_n(g)\right|>\eps\right)<\eta
			.\] 
			Let \(\calF_\delta = \left\{s=f-g:f,g\in\calF\text{ and }\rho(f,g)\leq \delta\right\}\). The above can be restated as showing that \(\exists\delta>0\) such that:
			\[
				\lim\sup_{n\to\infty}\P\left(\sup_{s\in\calF_\delta}\left|\mathbb{G}_n(s)\right|>\eps\right)<\eta
			.\] 
			By Markov's inequality we can bound the probability in the above display by 
			\[		\frac{1}{\eps}\E\left[\sup_{s\in\calF_\delta}\left|\mathbb{G}_n(s)\right|\right] \leq  \frac{K}{\eps} \int_{0}^{\delta}\sqrt{\log\calN\left(\eps,\calF,d\right)}\,d\eps  
			\]
			And then we can sent the RHS to 0 by sending \(\delta\downarrow 0\) as long as the integral on the RHS is finite. Asymptotic tightness plus convergence of marginals will give convergence to a tight element in \(\ell^\infty(\calF)\) by Theorem~\ref{thm:vdv1.5.4}. What remains is to show the conditions on \(\mathbb{G}\), separability and subgaussian.
	\end{enumerate}	
\end{remark*}
