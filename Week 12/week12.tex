%! TEX root = /Users/manunavjeevan/Documents/UCLA/Third Year/Reading Group/wcep.tex

\subsection{Delta Method}%
\label{subsec:delta-method}

We now cover the standard Delta Method (for ``fully differentiable'' functions). First we want to define some more general notions of differentiability.

\subsubsection{Differentiability}%
\label{subsubsec:differentiability}

Recall that a function \(f:\SR\to\SR\) is differentiable at a point \(x_0\) if the limit
\[
	\lim_{h\to0} \frac{f(x_0+h)-f(x)}{h} 
.\] 
exists. In this case we call the value of the limit the derivative of \(f\) at \(x_0\) and denote this as \(f'(x_0)\). The derivative is useful as it gives a linear approximation of \(f\) in a neighborhood of \(x_0\), that is the derivative can instead be written as a scalar  \(f'(x_0)\) such that
\[
	 \lim_{h\to 0}\frac{|f(x+h)-f(x)-f'(x)h|}{h} = 0 
.\] 
or, more familiarly, \(f(x+h)-f(x) = f'(x)h + o(h)\). This linear approximation property is the useful bit that we will use in econometrics, so we want notions of derivatives in general spaces to reflect this.
\begin{example*}
	Suppose \(\sqrt{n}\left(\hat\theta-\theta_0\right)\overset{L}{\to}N(0,\sigma^2)\). We want to get the asymptotic distribution of \(\hat\theta^2-\theta_0^2\). We'll use the derivative of \(f(x)=x^2; f'(x)=2x\) evaluated at \(\theta_0\) to say that 
	\[
		\sqrt{n}\left(\hat\theta^2-\theta_0^2\right) = 2\theta_0\sqrt{n}\left(\hat\theta-\theta_0\right) + o_p(1)
	,\]
	by taking \(h = \hat\theta-\theta_0\). Now note that we know the distribution of \(2\theta_0\sqrt{n}\left(\hat\theta-\theta_0\right) \overset{L}{\to} N(0,4\theta-0^2\sigma^2)\). We have leveraged the linearity property.
\end{example*}

Let's start by generalizing this property to functions in \(\SR^k\). This will allow us to differentiate between fully differentiable vs directionally differentiable functions, an important distinction later on. All the definitions below will reflect the linearity property that  we desire.

\begin{definition}[Differentiabiltiy in \(\SR^k\)]
	\label{def:rk-differentiability}
	A function \(f:\SR^k\to\SR^m\) is differentiable at \(x_0\in\SR^k\) if there exists a linear transformation \(f'(x_0):\SR^k\to\SR^m\) such that\footnote{That is there is a \(m\times k\) matrix  \(A_{x_0}\)} 
	\begin{equation}
		\label{eq:rk-differntiability}
		\lim_{h\to 0}\frac{\|f(x+h)-f(x)-f'(x)h\|}{\|h\|}=0 
	.\end{equation} 
\end{definition}
Note that this is very similar to our notion of differentiability from before. Let's see how this contrasts with our familiar notion of partial differentiability and consider a function that has a gradient (i.e partial derivatives in the standard unit vector directions) but that we would not consider fully differentiable according to \defnref{def:rk-differentiability}.
\begin{example*}[Directionally but not Fully Differentiable]
	Consider the function \(f:\SR^k \to \SR\) given by:
	\[
		f(x,y) = 
		\begin{cases}
			\frac{x^2}{x+y}  & \text{if }(x,y)\neq (,0) \\
			0 &\text{if }(x,y) = (0,0)
		\end{cases}	
	.\]
	Let's consider the partial derivatives at \((0,0)\). Note that \(f(x,0) = x\) and  \(f(0,y) = 0\) so that
	\[
		\frac{\partial f}{\partial x}(0,0) = 1 \andbox \frac{\partial f}{\partial y}(0,0) = 0 \implies \nabla f(0,0) = \begin{bmatrix} 1 & 0 \end{bmatrix} 
	.\] 
	We might be tempted then to say that the gradient \(\nabla f(0,0)\) satisfies the requirements of the linear map  \(f'(0,0)\) in Definition~\ref{def:rk-differentiability}. However, let's consider approaching \((0,0)\) in the direction  \((h,h)\) for a real number \(h\to 0\). Note that \(f(h,h) = \frac{h}{2}\). Let's consider the limit in this direction
	\begin{align*}
		\lim_{h\to 0}\frac{|f(h,h) - f(0,0) - \nabla f(0,0)\cdot(h,h)|}{|(h,h)|} &= \lim_{h\to 0}\frac{|h/2 - h|}{|\sqrt{2}h|} = \frac{1}{2\sqrt{2}}\neq 0  
	.\end{align*} 
	So the gradient does not satisfy the conditions of Definition~\ref{def:rk-differentiability}. In fact, no linear map will and so that function is not differentiable at \((0,0)\).
\end{example*}
\begin{remark*}
	The notion of differentiability that we want requires the \textit{same} linear approximation to work uniformly (in all directions). But the partial derivatives that we are used to only look in one direction at a time. The existence of a gradient is necessary but not sufficient for Definition~\ref{def:rk-differentiability}.	
\end{remark*}

Let's generalize these definitions to general spaces. In general, let \((A,\|\cdot\|_A)\) and \((B,\|\cdot\|_B)\) be Banach spaces (complete normed vector spaces).\footnote{In general, I think we only need metrizable topological spaces. This is what is said in Andres' notes and in Van Der Vaart and Wellner. However, all the definitions below are given in terms of norms so to avoid confusion we'll just assume these are complete normed spaces.} 

\begin{definition}[Fréchet Differential]
	\label{def:frechet-differentiable}
	We say a function \(f:A\to B\) is Fréchet differentiable at a point \(x_0\in A\) if there exists a continuous linear function \(f_{x_0}':A\to B\) such that
	\[
		\lim_{\|h\|_A\to 0} \frac{\left\|f(x_0+h)-f(x_0)-f_{x_0}'(h)\right\|_B}{\|h\|_A} = 0 
	.\] 
	If this is the case we call the linear map \(f_{x_0}'\) the \textit{Fréchet Differential} at \(x_0\). 
	We can alternatively formulate this
	\[
		\lim_{\eps\to 0}\;\sup_{h\in S} \frac{\|f(x+\eps_h)-f(x)-\eps f_{x_0}'(h)\|_b}{\eps} = 0 
	.\] 
	for all bounded (finite diameter) sets \(S \subset A\).
\end{definition}
\begin{example*}[Fréchet Differential]
	Let \((A,\|\cdot\|_{\infty}) = (L^\infty, \|\cdot\|_\infty)\) and \((B,\|\cdot\|_{B}) = (\SR,|\cdot|)\). For a point \(t_0 \in T\) and a function \(x:T\to\SR \in L^\infty\), let \(f(x) = x(t_0)^2\). Fix \(x\) and consider the linear map on \(L^\infty\) into \(\SR\), \(f_x'(h) = 2x(t_0)h(t_0)\). Then
	\begin{align*}
		\lim_{ \|h\|_A\to 0} \frac{\left\|f(x+h)-f(x)-f_x'(h)\right\|_B}{\|h\|_A} &= \lim_{\|h\|\to 0}\frac{\left|\left(x(t_0)+h(t_0)^2\right)-x(t_0)^2 - 2x(t_0)h(t_0)\right|}{\|h\|_\infty}  	\\ 
		&= \lim_{\|h\|_\infty \to 0}\frac{h(t_0)^2}{\|h\|_\infty} \\
		&\leq \lim_{\|h\|_\infty\to0}\|h\|_\infty = 0
	\end{align*}
\end{example*}

This generalizes the concept of a fully differentiable function, but what about directionally differentiable functions? For those we consider the Gateaux Differential. 
\begin{definition}[Gateaux Differential]
	\label{def:gateaux-differential}
	A function \(f:A\to B\) is Gateaux differentiable at the point \(x_0\in A\) in the direction \(h\in A\) if there exists a linear map \(\Gamma_{x_0}:A\to B\) such that 
	\[
		\lim_{\eps\to0} \frac{\left\|f(x+\eps h)-f(x) - \eps\Gamma_x(h)\right\|_B}{\eps} = 0 
	.\] 
	In this case we call the map \(\Gamma_x(\cdot)\) the \textit{Gateaux Differential} in the direction \(h\) and denote \(df(x_0;h)=\Gamma_x(\cdot)\).
\end{definition}
Note that this is defined in each direction as opposed the Fréchet differential which is defined uniformly for all directions. Whenever the Fréchet differential exists the Gateaux differential will exist and coincide with the Fréchet. 

The definitions of Fréchet differentiability is not quite what we need however. The following refinement becomes more useful to applications in econometrics.

\begin{definition}[Hadamard Differential]
	\label{def:hadamard}
	The function \(f:A\to B\) is Hadamard differentiable at the point \(x_0\in A\) if there exists a continuous linear function \(f_{x_0}': A \to B\) with 
	\[
		\lim_{\eps\to 0}\sup_{h\in S} \frac{\left\|f(x+\eps h)-f(x)- \eps f_{x_0}'(h)\right\|_B}{\eps} =0
	.\]
	for all compact sets \(S\subset A\). If this is the case the continuous linear function \(f_{x_0}'\) is called the \textit{Hadamard Differential} at \(x_0\). 
\end{definition}

The key idea here is that tight random variables concentrate on compact sets, so Hadamard is what we need.

\subsubsection{Standard Delta Method}%
\label{subsubsec:standard-delta-method}

We are now ready to review the Delta Method. This discussion follows Andres' notes as well as Chapter 3.9 in Van der Vaart and Wellner. 

First recall some useful theorems.

\begin{theorem}[Continuous Mapping Theorem]
	\label{thm:cmt-2}
	Suppose \(g_n:D\to E\) is a sequence of continuous maps with \(g_n(x_n)\to g(x)\) for some continuous map \(g\) and every convergent sequence \(x_n \to x\). Then if \(X_n\overset{L}{\to} X\)	in \(D\) then  \(g_n(X_n)\overset{L}{\to} g(X )\) in \(E\).
\end{theorem}

This is a slight refinement of the continuous mapping theorem seen in Theorem~\ref{thm:cmt}, allowing for sequences of continuous maps.
\begin{lemma}[Convergent Sequences are Compact]
	\label{lemma:convergent-compact}
	If a sequence \(\{x_n\}\) converges to a point \(x\), then the set  \( \left\{x,x_1,x_2,\dots\right\}\) is compact (in any topological space).
\end{lemma}
\begin{proof}
	Let \(\{U_i\}_{i\in I}\) be an open cover of \(S = \{x,x_1,x_2,\dots\} \). Pick a set \(U_x\) in  \(\{U_i\}_{i\in I}\) such that \(x\in U_x\). This is an open neighborhood of \(x\) so there must exist a number \(N\) such that for all \(n \geq N\), \(x_n \in U_x\). For the finitely many points outside of  \(U_x\) we can find sets in \(\{U_i\}_{i\in I}\) that contain them. 
\end{proof}

We are now ready to show the Delta Method. 
\begin{theorem}[Delta Method]
	\label{thm:delta-method}
	Let \(D\) and  \(E\) be Banach Spaces and \(\phi: D\to E\) be Hadamard differentiable at  \(\theta_0\) and suppose  \( \sqrt{n}(X_n-\theta_0)\overset{L}{\to} X\) in \(D\). Then \(\sqrt{n}\left(\phi(X_n)-\phi(\theta_0)\right)\overset{L}{\to}\phi'_{\theta_0}(X)\) where \(\phi'_{\theta_0}\) is the Hadamard Differential at \(\theta_0\).
\end{theorem}
\begin{proof}
	The goal will be to apply Theorem~\ref{thm:cmt-2}.
	Let \(g_n(h) = \sqrt{n}\left(\phi(\theta_0+\frac{h}{\sqrt{n}})-\phi(\theta_0)\right)\) and let \(g(h) = \phi'_{\theta_0}(h)\) and suppose  \(h_n\to h\). We want to show that \(g_n(h_n) \to g(h)\).
	\begin{align*}
		\lim_{n\to \infty} |g_n(h_n) - g(h)| 
		&= \lim_{n\to \infty} \left|\sqrt{n}\left(\phi\left(\theta_0+h_n/\sqrt{n}\right) - \phi(\theta_0)\right) - \phi_{\theta_0}'(h) \right| \\
		\intertext{Fix \(\eps_n=1/\sqrt{n}\) and rewrite}
		&= \lim_{n\to\infty} \left|\frac{1}{\eps_n}\left[\phi(\theta_0+h_n\eps_n)-\phi(\theta_0)- \eps_n \phi_{\theta_0}'(h)\right]\right|\\ 
		&\leq  \lim_{n\to\infty} \left|\frac{1}{\eps_n}\left[\phi\left(\theta_0 + h_n\eps_n)\right) - \phi(\theta_0)-\eps_n\phi_{\theta_0}'(h_n)\right]\right| + \underbrace{\left|\phi'_{\theta_0}(h_n) - \phi'_{\theta_0}(h)\right|}_{\to 0\text{ by continuity of }\phi'_{\theta_0}} \\
		\intertext{Since the last term goes to 0, consider only the first term and let \(S = \{h,h_1,h_2,\dots\}\):}
		&\leq \lim_{\eps_n\to 0} \sup_{\tilde h\in S} \left|\frac{1}{\eps_n}\left[\phi(\theta_0+\tilde h\eps_n)-\phi(\theta_0)- \eps_n\phi_{\theta_0}'(\tilde h)\right]\right|
	\end{align*}
	By Lemma~\ref{lemma:convergent-compact} the set \(S\) is compact and so this goes to zero because \(\phi\) is Hadamard differentiable at \(\theta_0\). We can now apply Theorem~\ref{thm:cmt-2} to show that \(g_n\left(\sqrt{n}(X_n-\theta_0)\right)\overset{L}{\to} g(X)\). Plugging in we see that
	\begin{align*}
		g_n\left(\sqrt{n}(X_n-\theta_0)\right) 
		&= \sqrt{n}\left(\phi\left(\theta_0+\frac{\sqrt{n}(X_n-\theta_0)}{\sqrt{n}} \right)-\phi(\theta_0)\right)\\
		&= \sqrt{n}\left(\phi(X_n)-\phi(\theta_0)\right) \\
		&\overset{L}{\to}\phi_{\theta_0}'(X)
	\end{align*}
\end{proof}
\subsubsection{Delta Method Examples}%
\label{subsubsec:delta-method-examples}

We now turn to some examples to show the usefulness of Theorem~\ref{thm:delta-method}.
\begin{example}[Regression Coeffecients]
	\label{ex:beta-hat-delta-method}
	Suppose \(\beta_0\in\SR^k\) and \( \sqrt{n}(\widehat\beta-\beta_0)\overset{L}{\to}N(0,\Sigma)\). Let \(\phi:\SR^k\to\SR^m\) be differentiable.\footnote{In \(\SR^k\) Fréchet and Hadamard differentiability are equivalent.} Then \(\sqrt{n}(\phi(\hat\beta)-\phi(\beta))\overset{L}{\to}\phi_{\beta_0}'(Z)\) where \(Z\sim N(0,\Sigma)\).

	What does this mean? Recall that any continuous linear map from \(\SR^k\) to \( \SR^m\) can be expressed as a matrix (an element of \(\SR^{m\times k}\)). Specifically we can write \(\phi = \left(\phi_1,\dots,\phi_m\right)'\), where each \(\phi_i:\SR^k\to \SR\). Then \(\phi'_{\beta_0} = \left(\nabla\phi_1(\beta_0),\dots,\nabla\phi_m(\beta_0)\right)' \in \SR^{m\times k}\). Then
	\[
		\phi_{\beta_0}'(Z) \sim N\left(0,\phi_{\beta_0}'\Sigma\phi_{\beta_0}\right)
	.\]
\end{example}
\begin{example}[Uniform Semi-Parametric Inference]
	\label{ex:semi-parametric-delta-method}
	Suppose  \(Y=m(X,\beta_0)+\eps\) where \(\E[\eps|X]=0\) and \(\beta\in\SR^K\) and \(\{Y_i,X_i\}\) an i.i.d sample. Under usual assumptions we will get that  \(\sqrt{n}(\widehat\beta-\beta_0)\overset{L}{\to}N(0,\Sigma)\). To forecast \(\E[Y|X=x_0]=m(x_0,\beta_0)\) we want to use \(m(x_0,\widehat\beta)\). If \(m(x_0,\cdot)\) is differentiable we can use Delta Method (\thmref{thm:delta-method}) to show that \(\sqrt{n}(m(x_0,\widehat\beta)-m(x_0,\beta_0))\overset{L}{\to}\nabla_\beta m(x_0,\beta_0) Z\) where \(Z\sim N(0,\Sigma)\).

	This is good for inference at a single point \(x_0\), but what if we want a uniform confidence interval for \(E[Y|X=x]\) for all points  \(x\) in some set \(X\). Let  \(\phi:\SR^k\to L^\infty(X)\) be given by \(\phi(\beta)=m(\cdot,\beta)\). That is, for each  \(\beta\) we can give an  \(m(x,\beta)\) for each point  \(x\in X\). This defines a function on  \(L^\infty(X)\). Assume that  \(\sup m(\cdot, \cdot)\) and  \(\nabla_\beta m(\cdot,\cdot)\) are continuous and  \(X\) is compact. Our guess for  \(\phi_{\beta_0}'\) is just  \(\phi_{\beta_0}'(\beta) =\nabla_\beta m(\cdot,\beta_0)\beta\). 
	\begin{enumerate}
		\item \(\phi_{\beta_0}':\SR^k\to L^\infty(X)\) is just like \(\phi:\SR^k\to L^{\infty}\), both take in a \(\beta\) and return a function on  \(X\). 
		\item Clearly \(\phi_{\beta_0}'\) is linear  (in  \(\beta\)) and if  \(\beta_n\to\beta\) in  \(\SR^k\) then \(\phi'_{\beta_0}(\beta_n)\to \phi'_{\beta_0}(\beta)\) in  \(L^\infty(X)\). In other words, \(\phi_{\beta_0}'\) is continuous.
		\begin{align*}
			\|\phi'_{\beta_0}(\beta_n)-\phi_{\beta_0}'(\beta)\|_\infty 
			&= \sup_{x\in X} \|\nabla_\beta m(x,\beta_0)(\beta_n-\beta)\| \leq \sup_{x\in X}\underbrace{\|\nabla_\beta m(x,\beta_0)\|}_{\text{ finite by compactness}}\cdot  \underbrace{\vphantom{\nabla_\beta}\|\beta_n-\beta\|}_{\to\, 0}
		\end{align*}
	\end{enumerate}
	Given this guess for \(\phi_{\beta_0}'\) lets check Hadamard Differentiability (\defnref{def:hadamard}) at \(\beta_0\). Let \(B\) be an arbitrary compact set in  \(\SR^k\):
	\begin{align*}
		\lim_{\eps_n\to 0}\sup_{h\in B} \frac{\|\phi(\beta_0+\eps_nh)-\phi(\beta_0) -\eps_n\phi'_{\beta_0}(h)\|_\infty}{\eps_n} 
		&= \lim_{\eps_n\to 0}\sup_{h\in B}\frac{\left\|m(x,\beta_0+\eps_nh)-m(x,\beta_0)-\eps_n\nabla_\beta m(x,\beta_0)h\right\|_\infty}{\eps_n}\\
		\intertext{By mean value theorem, for some \(\bar{\beta}(x)\in [\beta_0, \beta_0+ \eps_nh]\):}
		&= \lim_{\eps_n\to0}\sup_{h\in B}\sup_{x\in X}\left|\nabla_\beta m(x,\bar\beta(x))h-\nabla_\beta m(x,\beta_0)h\right|\\
		&\leq \lim_{\eps_n\to 0}\sup_{h\in B}\sup_{x\in X}\|\nabla_\beta m(x,\bar\beta(x))-\nabla_\beta m(x,\beta_0)\|\cdot\|h\|
	\end{align*}
	The first term on the right goes to zero uniformly for all \(x\in X\) by continuity of \(m(\cdot,\cdot)\) and compactness of  \(X\). The second term is bounded by compactness of  \(B\) and so the whole thing goes to 0. Since \(\phi:\SR^k\to L^\infty(X)\), \(\phi(\beta)=m(\cdot,\beta)\) is Hadamard Differentiable at \(\beta_0\) with \(\phi_{\beta_0}'(\beta) = \nabla_\beta m(\cdot,\beta_0)\beta\), the Delta Method (\thmref{thm:delta-method}) gives us that
	\[
		\sqrt{n}\left(\widehat\beta-\beta_0\right)\overset{L}{\to} Z \implies \sqrt{n}(\phi(\widehat\beta)-\phi(\beta_0))\overset{L}{\to} \phi'_{\beta_0}(Z)
	.\] 
	in other words,
	 \[
		 \sqrt{n}\left(m(\cdot, \widehat\beta)-m(\cdot,\beta_0)\right)\overset{L}{\to}\nabla_\beta m(\cdot,\beta_0)Z
	.\] 
	where the convergence is in \(L^\infty(X)\), that is uniformly over  \(x\in X\).
\end{example}

\begin{example}[Uniform Standard Deviation Estimation]
	\label{ex:uniform-variance-delta-method}
	Suppose we have a class of function of square integrable functions, \(\calF=\{f:\SR^k\to \SR\}\) and \(X\in \SR^k\) is a random variable such that \(\E[f(X)]=0\) for all \(f\in\calF\). Suppose for each \(f\in\calF\) we want to study the limiting behavior of the empirical standard deviation \[\sqrt{\frac{1}{n}\sum_{i=1}^n f^2(x_i)}\]
	By the delta method, since the derivative of \(\sqrt{x}\) is \(\frac{1}{2\sqrt{x}}\), if \(\sqrt{n}(\E_n f^2 -\E f^2)\to N(0,\sigma^2(f^2))\) then
	\[
		\sqrt{n}\left(\sqrt{\E_n f^2}-\sqrt{\E f^2}\right)\overset{L}{\to} N\left(0, \frac{1}{4}\sqrt{\sigma^2(f^2)}\right)	
	.\] 
	This is all good for a since function \(f\in \calF\), but suppose that we want to conduct inference uniformly over \(\calF\).  Let \(\calF^2 = \{f^2 : f\in\calF\}\). Assume that \(\calF^2\) is Donsker so that  \(\mathbb{G}_n\to\mathbb{G}\) for a tight element \(\mathbb{G}\) in \(L^\infty(\calF^2)\).  Also assume that \(0 < \inf_{f\in\calF}\E f^2<\sup_{f\in\calF}\E f^2<\infty\). 

	Let \(\phi:L^\infty(\calF^2)\to L^\infty(\calF^2)\) by \(\phi(G)(f^2) = \sqrt{G(f^2)}\). It is useful here to recall that \(G \in L^\infty(\calF^2)\) is a (bounded) function from  \(\calF^2\to\SR\). Let's consider applying \(\phi\) to the function  \(\theta_0(f)=\E f^2\) and guess that \(\phi_0'(G)(f) = \frac{1}{2\sqrt{\E f^2}}G(f)\), which is clearly linear. It is also easy to verify continuity since \(\E f^2\) is bounded from below uniformly for \(f\in\calF\). Let's verify that this function satisfies the property required of the Hadamard Differential (\defnref{def:hadamard}). Let \(S\) be a compact set in  \(L^\infty(\calF)\):
	\begin{align*}
		\lim_{\eps_n\to 0}\sup_{h \in S}\frac{\|\phi(\theta_0+h\eps_n)-\phi(\theta_0)-\phi_0'(h\eps_n)\|}{\eps_n} 
		&= \lim_{\eps_n\to 0}\sup_{h\in S} \sup_{f^2\in\calF^2}\frac{ \left|\sqrt{\theta_0(f^2)+\eps_nh(f^2)}-\sqrt{\theta_0(f^2)} - \eps_n\frac{h(f^2)}{2\sqrt{\theta_0(f^2)}} \right|}{\eps_n} \\
		\intertext{Again applying mean value theorem, for some \(\bar\theta \in [\theta_0, \theta_0+\eps_n h]\) (containment is pointwise):} 
		&= \lim_{\eps_n\to0}\sup_{h\in S}\sup_{f^2 \in \calF^2} \left|\frac{h(f^2)}{2\sqrt{\bar\theta_n(f^2)}} - \frac{h(f^2)}{2\sqrt{\theta_0(f^2)}}  \right| \\
		&\leq \lim_{\eps_n\to 0}\sup_{h\in S}\sup_{f^2\in\calF^2}\left|\frac{1}{2\sqrt{\bar\theta_n(f^2)}}-\frac{1}{2\sqrt{\theta_0(f^2)}}\right|\left|h(f^2)\right|
	\end{align*}
	The first term goes to zero uniformly (WHY?) and the second term is bounded uniformly because \(S\) is compact. This verifies Hadamard differentiability. Applying the Delta Method then gives us that 
	\[
		\sqrt{n}\left(\sqrt{\E_n f^2}-\sqrt{\E f^2}\right)\overset{L}{\to} \frac{\mathbb{G}(f^2)}{2\sqrt{\E f^2}}\;\;\;\;\text{ uniformly for \(f\in\calF\)}
	.\]
\end{example}
\begin{remark*} Comments on the delta method:
	\begin{itemize}
		\item Delta method is very powerful in infinite dimensions
		\item Lots of examples, e.j going from uniform inference on the empirical CDF to inference on the empirical quantile process.
		\item Domain and rules can be complicated, though have to be careful with norms.
	\end{itemize}
\end{remark*}

\subsection{Directionally Differentiable Functions}%
\label{subsec:directionally-differentiable}

The Delta Method in Theorem~\ref{thm:delta-method} works if the function \(\phi\) is Hadamard Differentiable. However, what if we only have  \textit{directional differentiability} of \(\phi\)? Let's first review what directionally differentiable means. 
\begin{definition}[Directional Hadamard Differential]
	\label{def:directional-hadamard}
	Let \(A\) and  \(B\) be Banach spaces and let  \(\phi: A_\phi \subseteq A \to B\). The map  \(\phi\) is Hadamard directionally differentiable at  \(\theta_0 \in A_{\phi}\) tangentially to a set  \(A_0 \subseteq A\) if there is a continuous map  \(\phi_{\theta_0}':A_0\to B\) such that 
	\begin{equation}
		\label{eq:directional-hadamard}
		\lim_{n\to \infty}\frac{\left\|\phi(\theta_0 + \eps_n h_n)-\phi(\theta_0) - \eps_n\phi_{\theta_0}'(h)\right\|_B}{\eps_n} = 0
	\end{equation}
	for all \(h_n \to h\in A_0\) and \(\eps_n\downarrow 0\). In this case, the map \(\phi'_{\theta_0}\) is called the \textit{Hadamard directional differential} at \(\theta_0\) tangent to \(A_0\).
\end{definition}
\begin{example}[Directional Differentiability]
	\label{ex:max-directional-differential}
	Let's consider the function \(\phi:\SR\to\SR_+, x\mapsto \max\{x,0\}\). We want to show that this function is Hadamard directionally differentiable tangent to \(\SR\) at any point \(x_0\in\SR\) with directional differential given by 
	\[
		\phi_{x_0}'(h) = \begin{cases}
			h & \text{if } x_0 > 0 \\
			\max\{h,0\} &\text{if } x_0=0\\
			0 & \text{if } x_0 < 0
		\end{cases}
	.\] 
	Intuitively, we can see why this would be the case. If \(x_0\) is above zero then locally in a neighborhood around \(x_0\), \(\phi(x)\) is equal to \(x\), if \(x_0\) equals zero then in any neighborhood around \(x_0\) \(\phi(x)\) is equal to \(x\) if \(x > 0\) and \(0\) otherwise, and if \(x_0\) is less than zero then \(\phi(x)\) is equal to zero uniformly in a neighborhood around \(x_0\).

	We can clearly see that this is a continuous function in \(h\) for any \(x_0\) so what remains is to verify \cref{eq:directional-hadamard}. Take any \(x_0\) and any sequence \(h_n\to h\) and \(\eps_n\downarrow 0\).
	\begin{align}
		\label{eq:ex-max-differential}
		\lim_{n\to\infty}\frac{\left|\max\{x_0 + \eps_nh_n, 0\} - \max\{x_0,0\} - \eps_n \phi_{\theta_0}'(h)\right|}{\eps_n}\tag{Ex-1}
	\end{align}
	Formally apply the intuition above: if \(x_0 > 0\) eventually we will have \(x_0 + \eps_nh_n > 0\). In this case \eqref{eq:ex-max-differential} will reduce to:
	\[
		\lim_{n\to\infty}\frac{\left|x_0 + \eps_nh_n - x_0 - \eps_n h\right|}{\eps_n} = \lim_{n\to\infty} h_n - h = 0 
	.\]
	If \(x_0 < 0\) then eventually we will have \(x_0 + \eps_nh_n <0\), then \eqref{eq:ex-max-differential} reduces to: 
	\[
		\lim_{n\to\infty} \frac{\left| -\eps_n \phi_{\theta_0}'(h)\right|}{\eps_n} = 0
	.\]
	Finally, if \(x_0 = 0\) then \eqref{eq:ex-max-differential} reduces to:
	\[
		\lim_{n\to\infty} \frac{\left|\max\{\eps_nh_n,0\} - \eps_n\max\{h,0\}\right|}{\eps_n} = \lim_{n\to\infty}\left|\max\{h_n,0\}-\max\{h,0\}\right| =0
	.\] 
	where in the first equality we use that \(\eps_n\) is decreasing towards zero and in the second we continuity of \(\max\{x,0\}\).
\end{example}
\begin{theorem}[Delta Method For Directionally Differentiable Functions]
	\label{thm:directional-delta-method}
	Suppose \(\sqrt{n}\left(\theta_n - \theta_0\right)\overset{L}{\to}X\) for a random element \(X\) in \(A_0\). If \(\phi:A\to B\) is Hadamard directionally differentiable at \(\theta_0\) tangent to \(A_0\) at \(\theta_0\) with  directional differential \(\phi'_{\theta_0}\) then:
	 \[
		 \sqrt{n}\left(\phi(\theta_n) - \phi(\theta_0)\right)\overset{L}{\to} \phi'_{\theta_0}(X)
	,\] 
\end{theorem}
\begin{proof}
	Proof follows the same steps as \Cref{thm:delta-method}. Again take \(g_n(h) = \sqrt{n}\left(\phi(\theta_0 + \frac{h}{\sqrt{n}})-\phi(\theta_0)\right)\) and \(g(h) = \phi_{\theta_0}'(h)\).We want to apply \Cref{thm:cmt-2} so we want to show that if \(h_n \to h \), \(g_n(h_n)\to g(h)\) for any sequence \(h_n\) converging to an \(h \in A_0\). Letting \(\eps_n=1/\sqrt{n}\) and following verbatim the first two lines in the proof of \Cref{thm:delta-method} gives us that
	\begin{align*}
	\lim_{n\to\infty}\left|g_n(h_n)-g(h)\right| = \lim_{n\to \infty}\frac{\left|\phi(\theta_0+\eps_nh_n)-\phi(\theta_0)-\eps_n\phi_{\theta_0}'(h)\right|}{\eps_n} 
	\end{align*}
	Applying the definition of Hadamard directional differentiability we see that this term goes to zero as \(\eps_n\to 0\) and \(h_n\to h\). Rest of the proof follows exactly that of \Cref{thm:delta-method}.
\end{proof}
\begin{example}[Delta Method for Directionally Differentiable Functions]
	Suppose \(\E[X^2] < \infty\) and we have an i.i.d sample from \(X\), \(X_1,X_2,\dots\) with \(X_i\sim X\). Let \(\theta_0 = \E[X]\) and \(\hat \theta_n = \frac{1}{n}\sum_{i=1}^n X_i\). By the Lindeberg Central Limit Theorem (\Cref{thm:lindeberg-clt}) we have that
	\[
		\sqrt{n}\left(\hat\theta_n-\theta_0\right)\overset{L}{\to} N(0,\sigma_X^2)
	.\] 
	Suppose are interested in conducting inference on the quantity \(\phi(\theta_0) = \max\{\theta_0,0\}\). In \Cref{ex:max-directional-differential} we verified that \(\phi(\cdot)\) is directionally differentiable everywhere tangent to \(\SR\) with Hadamard directional differential at (arbitrary point) \(x_0\):
	\[
		\phi_{x_0}'(h) = \begin{cases}
			h & \text{if }x_0 > 0 \\
			\max\{h,0\} & \text{if }x_0=0\\
			0 &\text{if }x_0 < 0
		\end{cases}
	.\] 
	So we can apply \Cref{thm:directional-delta-method} to get that
	\[
		\sqrt{n}\left(\max\{\hat\theta_n,0\} - \max\{\theta_0,n\}  \right)\overset{L}{\to} \phi_{\theta_0}'(\sigma_X Z)
	.\] 
	where \(Z\sim N(0,1)\).
\end{example}
