\subsection{Symmetrization}%
\label{subsec:symmetrization}

Symmetrization is a technique that will allow us to get/show(?) a subgaussian process. This follows the discussion in Chapter 2.2.1 and 2.3 in VanDerVaart and Wellner.

What sort of variables are subgaussian? A classic example below.

\begin{definition}[Rademachar Random Variable]
	\label{def:rademachar}
	Random variable \(\eps_i:\Omega_i\to\SR\) is a Rademachar random variable if
	\(\P(\eps_i=1)=\P(\eps_i=-1)=1/2.\) 
\end{definition}

The following lemma shows that a particular process consisting of Rademachar random variables is subgaussian.

\begin{lemma}[Hoeffding's Inequality]
	\label{lemma:hoeffding}
	Let \(a_1,\dots,a_n\) be constants and \(\eps_1,\dots,\eps_n\) be independent Rademachar random variables. Then 
	\[
		\P\bigg(\big|\sum_{i=1}^n a_i\eps_i\big| > x\bigg) \leq 2e^{\frac{1}{2}\frac{x^2}{\|a\|^2}  }
	.\] 
	where \(\|a\|\) denotes the Euclidean norm of \(a\).
\end{lemma}
\begin{proof}
	(From VdV\&W, Lemma 2.2.7) For any \(\lambda\) and any Rademachar random variable \(\eps\) one has that  \(\E e^{\lambda\eps} = \left(e^\lambda+e^{-\lambda}\right)/2\). By power series expansion:
	\begin{align*}
		e^\lambda &= 1 + \lambda + \frac{\lambda^2}{2!} + \frac{\lambda^3}{3!} +\dots \\ 
		e^{-\lambda} &= 1 - \lambda + \frac{\lambda^2}{2!} - \frac{\lambda^3}{3!}+\dots \\
		\implies \left(e^{\lambda} + e^{-\lambda}\right)/2 &= 1 + \frac{\lambda^2}{2!} + \frac{\lambda^4}{4!} + \frac{\lambda^6}{6!} \dots \\
														 &\leq 1 + \frac{\lambda^2}{2} + \frac{\lambda^4}{2^2\cdot2!} + \frac{\lambda^6}{2^3\cdot3!}   \\
														 &= e^{\lambda^2/2}
	\end{align*}
	where in the last inequality we use that \(2^k\cdot k! \leq (2k)!\) so that in total we have that \(\E e^{\lambda\eps} = \left(e^\lambda + e^{-\lambda}\right)/2 \leq e^{\lambda^2/2}\). Take \(\lambda = x/\|a\|^2\) and apply Markov's inequality to get the result.
\end{proof}

\begin{example*}
	For any functions \(f,g\) we have that
	\[
		\P\bigg(\bigg|\sum_{i=1}^n \frac{\eps_i}{\sqrt{n}}\left(f(x_i)-g(x_i)\right)\bigg| > x \mid \{X_i\}  \bigg) \leq 2e^{-\frac{1}{2}\frac{x^2}{d_n^2(f,g)}  }
	.\]
	where \(d_n^2(f,g):=\frac{1}{N}\sum_{i=1}^n \left(f(x_i)-g(x_i)\right)^2 \) is the square of the prediction norm.
\end{example*}

We would like to use the maximal inequality in Theorem~\ref{thm:vdv2.2.4} to control \(\E[\sup_{f,g}|\mathbb{G}_n(f)-\mathbb{G}_n(g)]\), but the problem is that \(\mathbb{G}_n\) is not (in general), subgaussian. However, from Lemma~\ref{lemma:hoeffding} we know that, at least conditional on our data, \(\mathbb{G}^\circ_n :=\frac{1}{\sqrt{n}}\sum \eps_i(f(x_i)-g(x_i)) \) is. Strategy will be to relate the two processes, \(\mathbb{G}_n\) and \(\mathbb{G}^\circ_n\).

\begin{lemma}[Symmetrization]
	\label{lemma:vdv2.3.1}
	For every nondecreasing, convex, \(\Phi:\SR\to\SR\) and class of measurable functions \(\calF\):
	\[
	    \E^\star\Phi\left(\left\|\P_n-P\right\|_\calF\right)\leq \E^\star\Phi\left(2\left\|\P_n^\circ\right\|_\calF\right)
	.\] 
\end{lemma}
