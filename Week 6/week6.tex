\subsection{Symmetrization}%
\label{subsec:symmetrization}

Symmetrization is a technique that will allow us to get/show(?) a subgaussian process. This follows the discussion in Chapter 2.2.1 and 2.3 in VanDerVaart and Wellner.

What sort of variables are subgaussian? A classic example below.

\begin{definition}[Rademachar Random Variable]
	\label{def:rademachar}
	Random variable \(\eps_i:\Omega_i\to\SR\) is a Rademachar random variable if
	\(\P(\eps_i=1)=\P(\eps_i=-1)=1/2.\) 
\end{definition}

The following lemma shows that a particular process consisting of Rademachar random variables is subgaussian.

\begin{lemma}[Hoeffding's Inequality]
	\label{lemma:hoeffding}
	Let \(a_1,\dots,a_n\) be constants and \(\eps_1,\dots,\eps_n\) be independent Rademachar random variables. Then 
	\[
		\P\bigg(\big|\sum_{i=1}^n a_i\eps_i\big| > x\bigg) \leq 2e^{\frac{1}{2}\frac{x^2}{\|a\|^2}  }
	.\] 
	where \(\|a\|\) denotes the Euclidean norm of \(a\).
\end{lemma}
\begin{proof}
	(From VdV\&W, Lemma 2.2.7) For any \(\lambda\) and any Rademachar random variable \(\eps\) one has that  \(\E e^{\lambda\eps} = \left(e^\lambda+e^{-\lambda}\right)/2\). By power series expansion:
	\begin{align*}
		e^\lambda &= 1 + \lambda + \frac{\lambda^2}{2!} + \frac{\lambda^3}{3!} +\dots \\ 
		e^{-\lambda} &= 1 - \lambda + \frac{\lambda^2}{2!} - \frac{\lambda^3}{3!}+\dots \\
		\implies \left(e^{\lambda} + e^{-\lambda}\right)/2 &= 1 + \frac{\lambda^2}{2!} + \frac{\lambda^4}{4!} + \frac{\lambda^6}{6!} \dots \\
														 &\leq 1 + \frac{\lambda^2}{2} + \frac{\lambda^4}{2^2\cdot2!} + \frac{\lambda^6}{2^3\cdot3!}   \\
														 &= e^{\lambda^2/2}
	\end{align*}
	where in the last inequality we use that \(2^k\cdot k! \leq (2k)!\) so that in total we have that \(\E e^{\lambda\eps} = \left(e^\lambda + e^{-\lambda}\right)/2 \leq e^{\lambda^2/2}\). Take \(\lambda = x/\|a\|^2\) and apply Markov's inequality to get the result.
\end{proof}

\begin{example*}
	For any functions \(f,g\) we have that
	\[
		\P\bigg(\bigg|\sum_{i=1}^n \frac{\eps_i}{\sqrt{n}}\left(f(x_i)-g(x_i)\right)\bigg| > x \mid \{X_i\}  \bigg) \leq 2e^{-\frac{1}{2}\frac{x^2}{d_n^2(f,g)}  }
	.\]
	where \(d_n^2(f,g):=\frac{1}{N}\sum_{i=1}^n \left(f(x_i)-g(x_i)\right)^2 \) is the square of the prediction norm.
\end{example*}

We would like to use the maximal inequality in Theorem~\ref{thm:vdv2.2.4} to control \(\E[\sup_{f,g}|\mathbb{G}_n(f)-\mathbb{G}_n(g)|]\), but the problem is that \(\mathbb{G}_n\) is not (in general), subgaussian. However, from Lemma~\ref{lemma:hoeffding} we know that, at least conditional on our data, \(\mathbb{G}^\circ_n :=\frac{1}{\sqrt{n}}\sum \eps_i(f(x_i)-g(x_i)) \) is. Strategy will be to relate the two processes, \(\mathbb{G}_n\) and \(\mathbb{G}^\circ_n\).

Before starting, it is useful to formally define the probability space that we are working with. Let \(\eps_1,\dots,\eps_n\) be i.i.d Rademachar random variables that are generated independent of \((X_1,\dots,X_n)\), our observed data. Define the symmetrized process:
\[
	\P_n^\circ f = \frac{1}{n}\sum_{i=1}^n \eps_if(X_i)
.\] 
Because \(\P_n^\circ\) is subgaussian, conditional on  \(X_1,\dots,X_n\), it can be easier to study. We want to bound supremum of the process \(\P_n-P\) by that of the symmetrized process. To formalize these bounds, we have to be careful about the nonmeasurability of supremum like \(\|\P_n - P\|_\calF\).\footnote{Even if \(\calF\) is a class of measurable functions, the supremum may not be measurable.} 

In the following discussion, outer expectations of functions of \(X_1,\dots,X_n\) are assumed to be taken with respect to the coordinate projection of the infinite product space \((\calX^\SN,\calA^\SN,P^\SN)\) onto its first \(n\) coordinates,  \((\calX^n,\calA^n, P^n)\).\footnote{That is the outer expectation is taken relative to \(P^n\)  where  \(P^n\) is defined from the projection of the infinite product space onto its first  \(n\) coordinates}. When auxilary variables, independent of the \(X\)'s are involved, as in the next lemma, we can use a similar convention. The underlying probability space is assumed to be of the form \((\calX^n,\calA^n,P^n)\times(\calZ,\calC,Q)\). Independence is understood in terms of a product probability space.\footnote{Two sub-sigma algebras, \(\calA_1, \calA_2 \subset\calA\) are considered independent if \(\P(A_1A_2) = \P(A_1)\P(A_2)\) for any  \(A_1 \in \calA_1,A_2\in\calA_2\). The sigma algebra generated by a
random map \(X:\left(\Omega,\calA,\P\right)\to(\calX,\calB)\) is the smallest sigma algebra on \(\Omega\) that makes  \(X\) measurable, 
\[
	\sigma(X) := \{X^{-1}(B):B\in\calB\} 
.\] 
Two random variables, \(X,Y\), defined on the same probability space are independent if their generated sigma algebras, \(\sigma(X),\sigma(Y)\), are independent. In the context of having independent draws \(X_1,\dots,X_n\) we can think of this as the projection mappings \(\pi_i(\calX^n)\) being independent. 
}. To manage all this, we take advantage of a modified Fubini's theorem for outer expectations, stated here without proof. 
\begin{lemma}[Fubini's Theorem, Lemma 1.2.6 VdV\&W]
	\label{lemma:vdv1.2.6}
	Let \(T\) be defined on a product probability space. Then
	 \[
		 \E_\star T\leq \E_{1\star}\E_{2\star}T\leq \E_1^\star\E_2^\star T \leq \E^\star T
	.\] 
\end{lemma}
\begin{proof}
	For the last inequality, we can assume that \(\E^\star T < \infty\) so that  \(\E^\star T = \E T^\star\). Since \(T^\star\) is jointly measurable with respect to the product  \(\sigma\)-field, the map  \(\omega_2\mapsto T^\star(\omega_1,\omega_2)\) is a measurable majorant of  \(\omega_2\mapsto T(\omega_1,\omega_2)\) for \(P_1\) almost all  \(\omega_1\). Hence \(\int T^\star(\omega_1,\omega_2)dP_2(\omega_2)\geq \left(\E_2^\star T\right)(\omega_1)\) for \(P_1\) almost all  \(\omega_1\). Further, by Fubini's theorem for standard integrals, this is a measurable function of \(\omega_1\). Thus the integral of this with respect to \(P_1\) is an upper bound for \(\E_1^\star\E_2^\star T\). Since  \(T^\star\) is jointly measurable, by another application Fubini's theorem for standard interals:
	 \[
		 \E^\star T = \E T^\star = \int\left(\int T^\star(\omega_1,\omega_2)dP_2(\omega_2)\right)dP_1(\omega_1)\geq \E_1^\star\E_2^\star T 
	.\]
	The inequalities for inner expectations hold by considering \(-T\).
\end{proof}


\begin{lemma}[Symmetrization]
	\label{lemma:vdv2.3.1}
	For every nondecreasing, convex, \(\Phi:\SR\to\SR\) and class of measurable functions \(\calF\):
	\[
	    \E^\star\Phi\left(\left\|\P_n-P\right\|_\calF\right)\leq \E^\star\Phi\left(2\left\|\P_n^\circ\right\|_\calF\right)
	.\]
	Where outer expectations are calculated as described above.
\end{lemma}
\begin{proof}
	Let \(Y_1,\dots,Y_n\) be independent copies of \(X_1,\dots,X_n\) (independently drawn from the same joint distribution as \(X_1,\dots,X_n\), defined formally as the coordinate projections on the last \(n\) coordinates in the product space  \((\calX^n,\calA^n,P^n)\times(\calZ,\calC,Q)\times(\calX^n,\calA^n,P^n)\)). 

	For \underline{fixed} values \(X_1,\dots,X_n\) applying Jensen's inequality to the absolute value gives:
	\[
		\left\|\P_n-P\right\|_\calF = \sup_{f\in\calF}\frac{1}{n}\left|\sum_{i=1}^n \big[f(X_i)-\E f(Y_i)\big]\right|\leq \E^\star_Y \sup_{f\in\calF} \frac{1}{n}\left|\sum_{i=1}^n \big[f(X_i)-f(Y_i)]\right| 
	.\] 
	where \(\E_Y^\star\) is the outer expectation with respect to \(Y_1,\dots,Y_n\) computed for \(P^n\). Again applying Jensen's inequality gives:
	 \[
		 \Phi\left(\left\|\P_n-P\right\|_\calF\right)\leq \E_Y\Phi\left(\bigg\|\frac{1}{n}\sum_{i=1}^n \big[f(X_i)-f(Y_i)] \bigg\|_\calF^{\star Y}\right)
	.\]
	where \(f^{\star Y}\) is the minimal measurable majorant of  \(f\) with respect to the distribution of  \(Y\). Because  \(\Phi\) is nondecreasing and continuous, the  \(\star Y\) inside  \(\Phi\) can be moved to  \(\E^\star_Y\). In total then:	 \[
		\Phi\left(\left\|\P_n-P\right\|_\calF\right)\leq \E_Y^{\star}\Phi\left(\bigg\|\frac{1}{n}\sum_{i=1}^n \big[f(X_i)-f(Y_i)] \bigg\|_\calF\right)
	.\]
	Next, take the expectation with respect to \(X_1,\dots,X_n\) of the above quantity to get:
	\[
		\E^\star\Phi\left(\|\P_n-P\|_\calF\right) \leq \E_X^\star\E_Y^\star\Phi\left(\frac{1}{n}\bigg\|\sum_{i=1}^n \big[f(X_i)-f(Y_i)\big]\bigg\| \right)
	.\]
	Adding a minus sign in front of the term \(\big[f(X_i)-f(Y_i)\big]\) has the effect of exchanging  \(X_i\) and \(Y_i\). By construction of the underlying probability space this does not change the expectation. Hence, the expression
	 \[
	    \E^\star\Phi\left(\frac{1}{n}\bigg\|\sum_{i=1}^n e_i\big[f(X_i)-f(Y_i)\big]\bigg\| \right)
	.\]
	is the same for any \(n\)-tuple  \((e_1,\dots,e_n)\in\{-1,1\}^n\). So:
	\[
		\E^\star\Phi\left(\big\|\P_n-P\big\|_\calF\right)\leq \E_\eps\E^\star_{X,Y}\Phi\left(\bigg\|\sum_{i=1}^n \eps_i\big[f(X_i)-f(Y_i)\big]\bigg\|_\calF\right)
	.\]
	where each \(\eps_i\) is an independent Rademachar random variable and  \(\eps = (\eps_1,\dots,\eps_n)\). By triangle inequality and convexity of the \(\Phi\):
	\begin{align*}
		\E_\eps\E^\star_{X,Y}\Phi\left(\bigg\|\sum_{i=1}^n \eps_i\big[f(X_i)-f(Y_i)\big]\bigg\|_\calF\right) 
		&\leq \E_\eps\E^\star_{X,Y}\Phi\left(\bigg\|\frac{1}{n}\sum_{i=1}^n \eps_if(X_i)\bigg\|_\calF + \bigg\|\frac{1}{n}\sum_{i=1}^n \eps_if(Y_i) \bigg\|_\calF\right)\\
		&\leq  \frac{1}{2}\E_\eps\E^\star_{X,Y}\Phi\left(2\bigg\|\frac{1}{n} \sum_{i=1}^n \eps_if(X_i)\bigg\|\right) + \frac{1}{2}\E_\eps\E^\star_{X,Y}\Phi\left(2\bigg\|\frac{1}{n} \sum_{i=1}^n \eps_if(Y_i)\bigg\|\right) \\
		&\leq \E^\star\Phi\left(2\left\|\P_n^\circ\right\|_\calF\right)
	\end{align*}
	where we use the face that a repeated outer expectation can be bounded above by a joint outer expectation, \(\E_\eps\E^\star_{X,Y} \leq \E^\star_{\eps,X,Y} (=\E^\star)\) using Lemma~\ref{lemma:vdv1.2.6}.
\end{proof}

